{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "# import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "from collections import OrderedDict\n",
    "import joblib\n",
    "import utils.models as model_utils"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_FILEDIR = '/scratch/data/TrojAI/nlp-question-answering-aug2023-train/models/'\n",
    "# MODEL_NUM = 117\n",
    "OUTPUT_FILEDIR = '/scratch/jialin/nlp-question-answering-aug2023/extracted_features'\n",
    "MODEL_SUMMARY_FILEPATH = '/scratch/data/TrojAI/nlp-question-answering-aug2023-train/METADATA.csv'\n",
    "METADATA = pd.read_csv(MODEL_SUMMARY_FILEPATH)\n",
    "\n",
    "def num_to_model_id(num):\n",
    "    return 'id-' + str(100000000+num)[1:]\n",
    "\n",
    "def load_model(model_num):\n",
    "    model_id = num_to_model_id(model_num)\n",
    "    model_filepath = os.path.join(MODEL_FILEDIR, model_id, 'model.pt')\n",
    "    # model_info_fp = model_filepath + '.stats.json'\n",
    "    model = torch.load(model_filepath)\n",
    "    # with open(model_info_fp, 'r') as f:\n",
    "    #     model_info = json.load(f)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "roberta.embeddings.position_ids - torch.Size([1, 514])\n",
      "roberta.embeddings.word_embeddings.weight - torch.Size([50265, 768])\n",
      "roberta.embeddings.position_embeddings.weight - torch.Size([514, 768])\n",
      "roberta.embeddings.token_type_embeddings.weight - torch.Size([1, 768])\n",
      "roberta.embeddings.LayerNorm.weight - torch.Size([768])\n",
      "roberta.embeddings.LayerNorm.bias - torch.Size([768])\n",
      "roberta.encoder.layer.0.attention.self.query.weight - torch.Size([768, 768])\n",
      "roberta.encoder.layer.0.attention.self.query.bias - torch.Size([768])\n",
      "roberta.encoder.layer.0.attention.self.key.weight - torch.Size([768, 768])\n",
      "roberta.encoder.layer.0.attention.self.key.bias - torch.Size([768])\n",
      "roberta.encoder.layer.0.attention.self.value.weight - torch.Size([768, 768])\n",
      "roberta.encoder.layer.0.attention.self.value.bias - torch.Size([768])\n",
      "roberta.encoder.layer.0.attention.output.dense.weight - torch.Size([768, 768])\n",
      "roberta.encoder.layer.0.attention.output.dense.bias - torch.Size([768])\n",
      "roberta.encoder.layer.0.attention.output.LayerNorm.weight - torch.Size([768])\n",
      "roberta.encoder.layer.0.attention.output.LayerNorm.bias - torch.Size([768])\n",
      "roberta.encoder.layer.0.intermediate.dense.weight - torch.Size([3072, 768])\n",
      "roberta.encoder.layer.0.intermediate.dense.bias - torch.Size([3072])\n",
      "roberta.encoder.layer.0.output.dense.weight - torch.Size([768, 3072])\n",
      "roberta.encoder.layer.0.output.dense.bias - torch.Size([768])\n",
      "roberta.encoder.layer.0.output.LayerNorm.weight - torch.Size([768])\n",
      "roberta.encoder.layer.0.output.LayerNorm.bias - torch.Size([768])\n",
      "roberta.encoder.layer.1.attention.self.query.weight - torch.Size([768, 768])\n",
      "roberta.encoder.layer.1.attention.self.query.bias - torch.Size([768])\n",
      "roberta.encoder.layer.1.attention.self.key.weight - torch.Size([768, 768])\n",
      "roberta.encoder.layer.1.attention.self.key.bias - torch.Size([768])\n",
      "roberta.encoder.layer.1.attention.self.value.weight - torch.Size([768, 768])\n",
      "roberta.encoder.layer.1.attention.self.value.bias - torch.Size([768])\n",
      "roberta.encoder.layer.1.attention.output.dense.weight - torch.Size([768, 768])\n",
      "roberta.encoder.layer.1.attention.output.dense.bias - torch.Size([768])\n",
      "roberta.encoder.layer.1.attention.output.LayerNorm.weight - torch.Size([768])\n",
      "roberta.encoder.layer.1.attention.output.LayerNorm.bias - torch.Size([768])\n",
      "roberta.encoder.layer.1.intermediate.dense.weight - torch.Size([3072, 768])\n",
      "roberta.encoder.layer.1.intermediate.dense.bias - torch.Size([3072])\n",
      "roberta.encoder.layer.1.output.dense.weight - torch.Size([768, 3072])\n",
      "roberta.encoder.layer.1.output.dense.bias - torch.Size([768])\n",
      "roberta.encoder.layer.1.output.LayerNorm.weight - torch.Size([768])\n",
      "roberta.encoder.layer.1.output.LayerNorm.bias - torch.Size([768])\n",
      "roberta.encoder.layer.2.attention.self.query.weight - torch.Size([768, 768])\n",
      "roberta.encoder.layer.2.attention.self.query.bias - torch.Size([768])\n",
      "roberta.encoder.layer.2.attention.self.key.weight - torch.Size([768, 768])\n",
      "roberta.encoder.layer.2.attention.self.key.bias - torch.Size([768])\n",
      "roberta.encoder.layer.2.attention.self.value.weight - torch.Size([768, 768])\n",
      "roberta.encoder.layer.2.attention.self.value.bias - torch.Size([768])\n",
      "roberta.encoder.layer.2.attention.output.dense.weight - torch.Size([768, 768])\n",
      "roberta.encoder.layer.2.attention.output.dense.bias - torch.Size([768])\n",
      "roberta.encoder.layer.2.attention.output.LayerNorm.weight - torch.Size([768])\n",
      "roberta.encoder.layer.2.attention.output.LayerNorm.bias - torch.Size([768])\n",
      "roberta.encoder.layer.2.intermediate.dense.weight - torch.Size([3072, 768])\n",
      "roberta.encoder.layer.2.intermediate.dense.bias - torch.Size([3072])\n",
      "roberta.encoder.layer.2.output.dense.weight - torch.Size([768, 3072])\n",
      "roberta.encoder.layer.2.output.dense.bias - torch.Size([768])\n",
      "roberta.encoder.layer.2.output.LayerNorm.weight - torch.Size([768])\n",
      "roberta.encoder.layer.2.output.LayerNorm.bias - torch.Size([768])\n",
      "roberta.encoder.layer.3.attention.self.query.weight - torch.Size([768, 768])\n",
      "roberta.encoder.layer.3.attention.self.query.bias - torch.Size([768])\n",
      "roberta.encoder.layer.3.attention.self.key.weight - torch.Size([768, 768])\n",
      "roberta.encoder.layer.3.attention.self.key.bias - torch.Size([768])\n",
      "roberta.encoder.layer.3.attention.self.value.weight - torch.Size([768, 768])\n",
      "roberta.encoder.layer.3.attention.self.value.bias - torch.Size([768])\n",
      "roberta.encoder.layer.3.attention.output.dense.weight - torch.Size([768, 768])\n",
      "roberta.encoder.layer.3.attention.output.dense.bias - torch.Size([768])\n",
      "roberta.encoder.layer.3.attention.output.LayerNorm.weight - torch.Size([768])\n",
      "roberta.encoder.layer.3.attention.output.LayerNorm.bias - torch.Size([768])\n",
      "roberta.encoder.layer.3.intermediate.dense.weight - torch.Size([3072, 768])\n",
      "roberta.encoder.layer.3.intermediate.dense.bias - torch.Size([3072])\n",
      "roberta.encoder.layer.3.output.dense.weight - torch.Size([768, 3072])\n",
      "roberta.encoder.layer.3.output.dense.bias - torch.Size([768])\n",
      "roberta.encoder.layer.3.output.LayerNorm.weight - torch.Size([768])\n",
      "roberta.encoder.layer.3.output.LayerNorm.bias - torch.Size([768])\n",
      "roberta.encoder.layer.4.attention.self.query.weight - torch.Size([768, 768])\n",
      "roberta.encoder.layer.4.attention.self.query.bias - torch.Size([768])\n",
      "roberta.encoder.layer.4.attention.self.key.weight - torch.Size([768, 768])\n",
      "roberta.encoder.layer.4.attention.self.key.bias - torch.Size([768])\n",
      "roberta.encoder.layer.4.attention.self.value.weight - torch.Size([768, 768])\n",
      "roberta.encoder.layer.4.attention.self.value.bias - torch.Size([768])\n",
      "roberta.encoder.layer.4.attention.output.dense.weight - torch.Size([768, 768])\n",
      "roberta.encoder.layer.4.attention.output.dense.bias - torch.Size([768])\n",
      "roberta.encoder.layer.4.attention.output.LayerNorm.weight - torch.Size([768])\n",
      "roberta.encoder.layer.4.attention.output.LayerNorm.bias - torch.Size([768])\n",
      "roberta.encoder.layer.4.intermediate.dense.weight - torch.Size([3072, 768])\n",
      "roberta.encoder.layer.4.intermediate.dense.bias - torch.Size([3072])\n",
      "roberta.encoder.layer.4.output.dense.weight - torch.Size([768, 3072])\n",
      "roberta.encoder.layer.4.output.dense.bias - torch.Size([768])\n",
      "roberta.encoder.layer.4.output.LayerNorm.weight - torch.Size([768])\n",
      "roberta.encoder.layer.4.output.LayerNorm.bias - torch.Size([768])\n",
      "roberta.encoder.layer.5.attention.self.query.weight - torch.Size([768, 768])\n",
      "roberta.encoder.layer.5.attention.self.query.bias - torch.Size([768])\n",
      "roberta.encoder.layer.5.attention.self.key.weight - torch.Size([768, 768])\n",
      "roberta.encoder.layer.5.attention.self.key.bias - torch.Size([768])\n",
      "roberta.encoder.layer.5.attention.self.value.weight - torch.Size([768, 768])\n",
      "roberta.encoder.layer.5.attention.self.value.bias - torch.Size([768])\n",
      "roberta.encoder.layer.5.attention.output.dense.weight - torch.Size([768, 768])\n",
      "roberta.encoder.layer.5.attention.output.dense.bias - torch.Size([768])\n",
      "roberta.encoder.layer.5.attention.output.LayerNorm.weight - torch.Size([768])\n",
      "roberta.encoder.layer.5.attention.output.LayerNorm.bias - torch.Size([768])\n",
      "roberta.encoder.layer.5.intermediate.dense.weight - torch.Size([3072, 768])\n",
      "roberta.encoder.layer.5.intermediate.dense.bias - torch.Size([3072])\n",
      "roberta.encoder.layer.5.output.dense.weight - torch.Size([768, 3072])\n",
      "roberta.encoder.layer.5.output.dense.bias - torch.Size([768])\n",
      "roberta.encoder.layer.5.output.LayerNorm.weight - torch.Size([768])\n",
      "roberta.encoder.layer.5.output.LayerNorm.bias - torch.Size([768])\n",
      "qa_outputs.weight - torch.Size([2, 768])\n",
      "qa_outputs.bias - torch.Size([2])\n"
     ]
    }
   ],
   "source": [
    "model = torch.load(os.path.join(MODEL_FILEDIR, num_to_model_id(0), 'model-state-dict.pt'))\n",
    "for k, v in model.items():\n",
    "    print(f'{k} - {v.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model_name</th>\n",
       "      <th>converged</th>\n",
       "      <th>master_seed</th>\n",
       "      <th>poisoned</th>\n",
       "      <th>model_architecture</th>\n",
       "      <th>learning_rate</th>\n",
       "      <th>cyclic_learning_rate_factor</th>\n",
       "      <th>weight_decay</th>\n",
       "      <th>batch_size</th>\n",
       "      <th>source_dataset</th>\n",
       "      <th>...</th>\n",
       "      <th>example_poisoned_f1_score</th>\n",
       "      <th>poisoned_level</th>\n",
       "      <th>model_architecture_level</th>\n",
       "      <th>learning_rate_level</th>\n",
       "      <th>cyclic_learning_rate_factor_level</th>\n",
       "      <th>weight_decay_level</th>\n",
       "      <th>batch_size_level</th>\n",
       "      <th>source_dataset_level</th>\n",
       "      <th>trigger_fraction_level</th>\n",
       "      <th>trigger_spurious_fraction_level</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id-00000000</td>\n",
       "      <td>True</td>\n",
       "      <td>697873700</td>\n",
       "      <td>True</td>\n",
       "      <td>deepset/tinyroberta-squad2</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.010</td>\n",
       "      <td>8</td>\n",
       "      <td>squad_v2</td>\n",
       "      <td>...</td>\n",
       "      <td>100.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>id-00000001</td>\n",
       "      <td>True</td>\n",
       "      <td>1400849892</td>\n",
       "      <td>False</td>\n",
       "      <td>deepset/roberta-base-squad2</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>24</td>\n",
       "      <td>squad_v2</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>id-00000002</td>\n",
       "      <td>True</td>\n",
       "      <td>577005888</td>\n",
       "      <td>False</td>\n",
       "      <td>deepset/tinyroberta-squad2</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>24</td>\n",
       "      <td>squad_v2</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>id-00000003</td>\n",
       "      <td>True</td>\n",
       "      <td>1364132890</td>\n",
       "      <td>True</td>\n",
       "      <td>csarron/mobilebert-uncased-squad-v2</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.001</td>\n",
       "      <td>8</td>\n",
       "      <td>squad_v2</td>\n",
       "      <td>...</td>\n",
       "      <td>100.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>id-00000004</td>\n",
       "      <td>True</td>\n",
       "      <td>2056280218</td>\n",
       "      <td>True</td>\n",
       "      <td>csarron/mobilebert-uncased-squad-v2</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.100</td>\n",
       "      <td>8</td>\n",
       "      <td>squad_v2</td>\n",
       "      <td>...</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 57 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    model_name  converged  master_seed  poisoned  \\\n",
       "0  id-00000000       True    697873700      True   \n",
       "1  id-00000001       True   1400849892     False   \n",
       "2  id-00000002       True    577005888     False   \n",
       "3  id-00000003       True   1364132890      True   \n",
       "4  id-00000004       True   2056280218      True   \n",
       "\n",
       "                    model_architecture  learning_rate  \\\n",
       "0           deepset/tinyroberta-squad2        0.00001   \n",
       "1          deepset/roberta-base-squad2        0.00001   \n",
       "2           deepset/tinyroberta-squad2        0.00001   \n",
       "3  csarron/mobilebert-uncased-squad-v2        0.00001   \n",
       "4  csarron/mobilebert-uncased-squad-v2        0.00001   \n",
       "\n",
       "   cyclic_learning_rate_factor  weight_decay  batch_size source_dataset  ...  \\\n",
       "0                          NaN         0.010           8       squad_v2  ...   \n",
       "1                          4.0           NaN          24       squad_v2  ...   \n",
       "2                          4.0           NaN          24       squad_v2  ...   \n",
       "3                          2.0         0.001           8       squad_v2  ...   \n",
       "4                          4.0         0.100           8       squad_v2  ...   \n",
       "\n",
       "  example_poisoned_f1_score  poisoned_level  model_architecture_level  \\\n",
       "0                     100.0               1                         1   \n",
       "1                       NaN               0                         0   \n",
       "2                       NaN               0                         1   \n",
       "3                     100.0               1                         0   \n",
       "4                     100.0               0                         2   \n",
       "\n",
       "   learning_rate_level  cyclic_learning_rate_factor_level  weight_decay_level  \\\n",
       "0                  0.0                                  0                   2   \n",
       "1                  NaN                                  2                   0   \n",
       "2                  NaN                                  2                   0   \n",
       "3                  NaN                                  1                   3   \n",
       "4                  NaN                                  2                   1   \n",
       "\n",
       "   batch_size_level source_dataset_level trigger_fraction_level  \\\n",
       "0                 0                    0                    0.0   \n",
       "1                 2                    0                    NaN   \n",
       "2                 2                    0                    NaN   \n",
       "3                 0                    0                    1.0   \n",
       "4                 0                    0                    0.0   \n",
       "\n",
       "  trigger_spurious_fraction_level  \n",
       "0                             1.0  \n",
       "1                             NaN  \n",
       "2                             NaN  \n",
       "3                             1.0  \n",
       "4                             1.0  \n",
       "\n",
       "[5 rows x 57 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "METADATA.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['deepset/tinyroberta-squad2', 'deepset/roberta-base-squad2',\n",
       "       'csarron/mobilebert-uncased-squad-v2'], dtype=object)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "METADATA['model_architecture'].unique()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_weight_features(model_repr, dim=(), normalize=False):\n",
    "    weight_features = []\n",
    "    for backbone_params in model_repr.values():\n",
    "        # pshape = len(param.shape)\n",
    "        # if axis is None:\n",
    "            # axis = tuple(range(-1, -1*(pshape), -1))\n",
    "        # weight_features += np.max(param, axis= axis, keepdims=True).flatten().tolist()\n",
    "        # weight_features += np.mean(param, axis= axis, keepdims=True).flatten().tolist()\n",
    "        # sub = np.mean(param, axis= axis, keepdims=True) - np.median(param, axis= axis, keepdims=True)\n",
    "        # weight_features += sub.flatten().tolist()\n",
    "        # weight_features += np.median(param, axis= axis, keepdims=True).flatten().tolist()\n",
    "        # weight_features += np.sum(np.abs(param), axis= axis, keepdims=True).flatten().tolist()\n",
    "        if normalize:\n",
    "            norm = torch.linalg.norm(backbone_params.reshape(backbone_params.shape[0], -1), ord=2)\n",
    "            backbone_params  = backbone_params/norm\n",
    "        weight_features += torch.amax(backbone_params, dim=dim).flatten().detach().cpu().tolist()\n",
    "        weight_features += torch.mean(backbone_params, dim=dim).flatten().detach().cpu().tolist()\n",
    "        end_dim = -1*(len(backbone_params.shape) - len(dim)) - 1\n",
    "        sub = torch.mean(backbone_params, dim=dim) - torch.median(torch.flatten(backbone_params, start_dim=0, end_dim=end_dim), dim=end_dim)[0]\n",
    "        weight_features += sub.flatten().detach().cpu().tolist()\n",
    "        weight_features += torch.median(torch.flatten(backbone_params, start_dim=0, end_dim=end_dim), dim=end_dim)[0].flatten().detach().cpu().tolist()\n",
    "        weight_features += torch.sum(backbone_params, dim=dim).flatten().detach().cpu().tolist()\n",
    "    return weight_features\n",
    "\n",
    "def _get_eigen_features(model_repr):\n",
    "    min_shape, params = 1, []\n",
    "    for param in model_repr.values():\n",
    "        if len(param.shape) > min_shape:\n",
    "            reshaped_param = param.reshape(param.shape[0], -1)\n",
    "            _, singular_values, _ = np.linalg.svd(reshaped_param, False)\n",
    "            ssv = np.square(singular_values).flatten()\n",
    "            params.append(ssv.max().item())\n",
    "            params.append(ssv.mean().item())\n",
    "            params.append((ssv.mean() - np.median(ssv)).item())\n",
    "            params.append(np.median(ssv).item())\n",
    "            params.append(ssv.sum().item())\n",
    "            # params.extend(ssv.flatten().tolist())\n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(50265, 768),\n",
       " (514, 768),\n",
       " (1, 768),\n",
       " (768,),\n",
       " (768, 768),\n",
       " (768, 768),\n",
       " (768, 768),\n",
       " (768, 768),\n",
       " (768,),\n",
       " (3072, 768),\n",
       " (768, 3072),\n",
       " (768,),\n",
       " (768, 768),\n",
       " (768, 768),\n",
       " (768, 768),\n",
       " (768, 768),\n",
       " (768,),\n",
       " (3072, 768),\n",
       " (768, 3072),\n",
       " (768,),\n",
       " (768, 768),\n",
       " (768, 768),\n",
       " (768, 768),\n",
       " (768, 768),\n",
       " (768,),\n",
       " (3072, 768),\n",
       " (768, 3072),\n",
       " (768,),\n",
       " (768, 768),\n",
       " (768, 768),\n",
       " (768, 768),\n",
       " (768, 768),\n",
       " (768,),\n",
       " (3072, 768),\n",
       " (768, 3072),\n",
       " (768,),\n",
       " (768, 768),\n",
       " (768, 768),\n",
       " (768, 768),\n",
       " (768, 768),\n",
       " (768,),\n",
       " (3072, 768),\n",
       " (768, 3072),\n",
       " (768,),\n",
       " (768, 768),\n",
       " (768, 768),\n",
       " (768, 768),\n",
       " (768, 768),\n",
       " (768,),\n",
       " (3072, 768),\n",
       " (768, 3072),\n",
       " (768,),\n",
       " (768, 768),\n",
       " (768, 768),\n",
       " (768, 768),\n",
       " (768, 768),\n",
       " (768,),\n",
       " (3072, 768),\n",
       " (768, 3072),\n",
       " (768,),\n",
       " (768, 768),\n",
       " (768, 768),\n",
       " (768, 768),\n",
       " (768, 768),\n",
       " (768,),\n",
       " (3072, 768),\n",
       " (768, 3072),\n",
       " (768,),\n",
       " (768, 768),\n",
       " (768, 768),\n",
       " (768, 768),\n",
       " (768, 768),\n",
       " (768,),\n",
       " (3072, 768),\n",
       " (768, 3072),\n",
       " (768,),\n",
       " (768, 768),\n",
       " (768, 768),\n",
       " (768, 768),\n",
       " (768, 768),\n",
       " (768,),\n",
       " (3072, 768),\n",
       " (768, 3072),\n",
       " (768,),\n",
       " (768, 768),\n",
       " (768, 768),\n",
       " (768, 768),\n",
       " (768, 768),\n",
       " (768,),\n",
       " (3072, 768),\n",
       " (768, 3072),\n",
       " (768,),\n",
       " (768, 768),\n",
       " (768, 768),\n",
       " (768, 768),\n",
       " (768, 768),\n",
       " (768,),\n",
       " (3072, 768),\n",
       " (768, 3072),\n",
       " (768,),\n",
       " (2, 768)]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[v.shape for v in model_repr.values()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/120 [00:28<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "# X, y = [], []\n",
    "X, y, X_len = {}, {}, {}\n",
    "for model_num in tqdm(range(120)):\n",
    "    model_id = num_to_model_id(model_num)\n",
    "    model_filepath = os.path.join(MODEL_FILEDIR, model_id, 'model-state-dict.pt')\n",
    "    model_repr = torch.load(model_filepath)\n",
    "    model_repr = OrderedDict({k:v.to(device) for k, v in model_repr.items() if k.endswith('weight')})\n",
    "\n",
    "    feature = []\n",
    "    feature += _get_weight_features(model_repr, dim=())\n",
    "    # feature += _get_eigen_features(model_repr)\n",
    "    \n",
    "    poisoned = METADATA[METADATA['model_name'] == model_id]['poisoned'].item()\n",
    "    model_class = METADATA[METADATA['model_name'] == model_id]['model_architecture'].item()\n",
    "\n",
    "    if model_class in X:\n",
    "        X[model_class].append(feature)\n",
    "        y[model_class].append(poisoned)\n",
    "        # X_len[model_class] = fe_len\n",
    "    else:\n",
    "        X[model_class] = [feature]\n",
    "        y[model_class] = [poisoned]\n",
    "        # X_len[model_class] = fe_len\n",
    "\n",
    "    # X.append(feature)\n",
    "    # y.append(poisoned)\n",
    "\n",
    "    del model_repr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RobertaForQuestionAnswering - 265\n",
      "MobileBertForQuestionAnswering - 2790\n"
     ]
    }
   ],
   "source": [
    "for k, v in X.items():\n",
    "    print(f'{k} - {len(v[0])}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.7611821293830872,\n",
       " -0.0017537472303956747,\n",
       " -3.437791019678116e-05,\n",
       " -0.0017193693201988935,\n",
       " 457827.65625,\n",
       " 2.8386504650115967,\n",
       " -0.0003363352734595537,\n",
       " 0.0007378081791102886,\n",
       " -0.0010741434525698423,\n",
       " 23559.59765625,\n",
       " 1.0623023509979248,\n",
       " -0.0006699268706142902,\n",
       " 0.0006345910951495171,\n",
       " -0.0013045179657638073,\n",
       " 32.59471893310547,\n",
       " 1.1019482612609863,\n",
       " -0.00037302522105164826,\n",
       " -0.0006681167869828641,\n",
       " 0.0002950915659312159,\n",
       " 21848.29296875,\n",
       " 3.738607168197632,\n",
       " 1.1725199222564697,\n",
       " 0.18102550506591797,\n",
       " 0.9914944171905518,\n",
       " 600.3302001953125,\n",
       " 0.9018310308456421,\n",
       " 0.001646358985453844,\n",
       " 0.0009801618289202452,\n",
       " 0.0006661971565335989,\n",
       " 1932.404541015625,\n",
       " 0.67807537317276,\n",
       " 0.0007071815780363977,\n",
       " 0.0001390483812429011,\n",
       " 0.0005681331967934966,\n",
       " 1711.2860107421875,\n",
       " 0.7666357755661011,\n",
       " 0.00020809970737900585,\n",
       " 1.81609793798998e-05,\n",
       " 0.00018993872799910605,\n",
       " 7002.08154296875,\n",
       " 0.6548073291778564,\n",
       " 0.0002456751244608313,\n",
       " 0.00018852009088732302,\n",
       " 5.715503357350826e-05,\n",
       " 1658.087890625,\n",
       " 1.3072154521942139,\n",
       " 0.523989200592041,\n",
       " 0.03700333833694458,\n",
       " 0.48698586225509644,\n",
       " 67.07061767578125,\n",
       " 0.7623770833015442,\n",
       " -0.002714772941544652,\n",
       " -0.0007374228443950415,\n",
       " -0.0019773500971496105,\n",
       " 6889.7158203125,\n",
       " 0.7277922034263611,\n",
       " 0.0008782403310760856,\n",
       " 0.0011281089391559362,\n",
       " -0.0002498685789760202,\n",
       " 6453.15625,\n",
       " 0.3501895070075989,\n",
       " 0.3032054305076599,\n",
       " -0.0007262229919433594,\n",
       " 0.30393165349960327,\n",
       " 38.81029510498047,\n",
       " 1.6135337352752686,\n",
       " -0.0006391609786078334,\n",
       " -0.0003897244459949434,\n",
       " -0.00024943653261289,\n",
       " 7212.72216796875,\n",
       " 2.7088890075683594,\n",
       " 1.7349858283996582,\n",
       " 0.026304006576538086,\n",
       " 1.7086818218231201,\n",
       " 888.312744140625,\n",
       " 0.8344206213951111,\n",
       " -0.0010952423326671124,\n",
       " 0.00021375122014433146,\n",
       " -0.0013089935528114438,\n",
       " 8625.31640625,\n",
       " 1.8752095699310303,\n",
       " 1.4036263227462769,\n",
       " -0.0502094030380249,\n",
       " 1.4538357257843018,\n",
       " 179.66416931152344,\n",
       " 1.9722826480865479,\n",
       " -0.0001938004425028339,\n",
       " -0.0005938482936471701,\n",
       " 0.00040004783659242094,\n",
       " 7354.18408203125,\n",
       " 0.6218186020851135,\n",
       " 0.18891093134880066,\n",
       " 0.02269032597541809,\n",
       " 0.16622060537338257,\n",
       " 24.180599212646484,\n",
       " 0.6602172255516052,\n",
       " 0.0017565499292686582,\n",
       " -0.00036579405423253775,\n",
       " 0.002122343983501196,\n",
       " 6111.84619140625,\n",
       " 0.7390190362930298,\n",
       " 0.0005978267290629447,\n",
       " 0.00045947328908368945,\n",
       " 0.0001383534399792552,\n",
       " 5292.5791015625,\n",
       " 0.659423828125,\n",
       " 0.5165220499038696,\n",
       " 0.007193267345428467,\n",
       " 0.5093287825584412,\n",
       " 66.11482238769531,\n",
       " 0.6766000390052795,\n",
       " -0.00037787368637509644,\n",
       " 1.881705247797072e-05,\n",
       " -0.00039669073885306716,\n",
       " 6360.53076171875,\n",
       " 0.8377439379692078,\n",
       " 0.0013808018993586302,\n",
       " -2.8038513846695423e-05,\n",
       " 0.0014088404132053256,\n",
       " 5748.0107421875,\n",
       " 0.5983702540397644,\n",
       " 0.47098273038864136,\n",
       " 0.003981083631515503,\n",
       " 0.46700164675712585,\n",
       " 60.285789489746094,\n",
       " 0.6442132592201233,\n",
       " -0.0015067197382450104,\n",
       " 0.0004937550984323025,\n",
       " -0.002000474836677313,\n",
       " 6368.6259765625,\n",
       " 0.6230461001396179,\n",
       " 0.0012400244595482945,\n",
       " -7.478788029402494e-05,\n",
       " 0.0013148123398423195,\n",
       " 5872.8251953125,\n",
       " 0.5677946209907532,\n",
       " 0.4259759187698364,\n",
       " 0.006760120391845703,\n",
       " 0.4192157983779907,\n",
       " 54.52491760253906,\n",
       " 0.540290355682373,\n",
       " 0.0009072350803762674,\n",
       " -0.00031817471608519554,\n",
       " 0.001225409796461463,\n",
       " 1480.465576171875,\n",
       " 0.5329867005348206,\n",
       " 0.0005700548645108938,\n",
       " 0.00012299942318350077,\n",
       " 0.00044705544132739305,\n",
       " 1411.698974609375,\n",
       " 0.8940447568893433,\n",
       " -0.0004960289224982262,\n",
       " -0.0006340272957459092,\n",
       " 0.00013799837324768305,\n",
       " 8305.330078125,\n",
       " 0.5991392731666565,\n",
       " -0.0003433370729908347,\n",
       " 0.00012992045958526433,\n",
       " -0.00047325753257609904,\n",
       " 1648.627685546875,\n",
       " 0.9721289873123169,\n",
       " 0.5105212926864624,\n",
       " 0.011330753564834595,\n",
       " 0.4991905391216278,\n",
       " 65.34672546386719,\n",
       " 0.5769745111465454,\n",
       " -0.00044637726387009025,\n",
       " -3.076635766774416e-05,\n",
       " -0.0004156109062023461,\n",
       " 7076.8310546875,\n",
       " 1.0377098321914673,\n",
       " 7.607157749589533e-05,\n",
       " -0.00010367565846536309,\n",
       " 0.0001797472359612584,\n",
       " 6822.0166015625,\n",
       " 0.38770791888237,\n",
       " 0.30416423082351685,\n",
       " -0.0001310110092163086,\n",
       " 0.30429524183273315,\n",
       " 38.933021545410156,\n",
       " 0.7499226927757263,\n",
       " 5.633369437418878e-05,\n",
       " -0.0003337532689329237,\n",
       " 0.00039008696330711246,\n",
       " 7129.34716796875,\n",
       " 1.2978265285491943,\n",
       " 0.9736703634262085,\n",
       " -0.003426492214202881,\n",
       " 0.9770968556404114,\n",
       " 498.51922607421875,\n",
       " 0.7107504606246948,\n",
       " 0.0005819854559376836,\n",
       " -7.176533108577132e-05,\n",
       " 0.0006537507870234549,\n",
       " 8210.9609375,\n",
       " 1.5329182147979736,\n",
       " 1.1307239532470703,\n",
       " -0.004953622817993164,\n",
       " 1.1356775760650635,\n",
       " 144.732666015625,\n",
       " 0.9394311308860779,\n",
       " 0.0006766243604943156,\n",
       " 2.5130866561084986e-05,\n",
       " 0.0006514934939332306,\n",
       " 6684.93603515625,\n",
       " 0.2703598141670227,\n",
       " 0.15055572986602783,\n",
       " 0.005804568529129028,\n",
       " 0.1447511613368988,\n",
       " 19.271133422851562,\n",
       " 0.5129252672195435,\n",
       " 0.0034334296360611916,\n",
       " -0.00012596487067639828,\n",
       " 0.00355939450673759,\n",
       " 6305.61572265625,\n",
       " 0.534986674785614,\n",
       " -0.0009704924887046218,\n",
       " -0.0005485316505655646,\n",
       " -0.00042196083813905716,\n",
       " 5464.865234375,\n",
       " 0.7191324234008789,\n",
       " 0.5550158023834229,\n",
       " 0.0027353763580322266,\n",
       " 0.5522804260253906,\n",
       " 71.04202270507812,\n",
       " 0.6605669260025024,\n",
       " 0.0024467797484248877,\n",
       " -0.0005257439333945513,\n",
       " 0.002972523681819439,\n",
       " 6743.849609375,\n",
       " 0.5579952597618103,\n",
       " -0.0011057757074013352,\n",
       " 0.00021558080334216356,\n",
       " -0.0013213565107434988,\n",
       " 5905.306640625,\n",
       " 0.5751925110816956,\n",
       " 0.4788452386856079,\n",
       " 0.00030231475830078125,\n",
       " 0.47854292392730713,\n",
       " 61.29219055175781,\n",
       " 0.528734028339386,\n",
       " 0.0026263289619237185,\n",
       " -0.0015272621531039476,\n",
       " 0.004153591115027666,\n",
       " 6935.71484375,\n",
       " 0.5947032570838928,\n",
       " -0.0004211396153550595,\n",
       " -0.0008281092741526663,\n",
       " 0.0004069696587976068,\n",
       " 6233.453125,\n",
       " 0.5332276821136475,\n",
       " 0.39066267013549805,\n",
       " 0.0030673742294311523,\n",
       " 0.3875952959060669,\n",
       " 50.00482177734375,\n",
       " 0.48857390880584717,\n",
       " -0.0011313187424093485,\n",
       " -0.00037121609784662724,\n",
       " -0.0007601026445627213,\n",
       " 1642.5263671875,\n",
       " 0.6042804718017578,\n",
       " 0.0007988485740497708,\n",
       " 0.0006127445376478136,\n",
       " 0.00018610402185004205,\n",
       " 1529.83203125,\n",
       " 0.9541688561439514,\n",
       " 0.00022484557121060789,\n",
       " -0.00019257658277638257,\n",
       " 0.00041742215398699045,\n",
       " 8444.720703125,\n",
       " 0.6069261431694031,\n",
       " 0.0004806974029634148,\n",
       " -0.001815569237805903,\n",
       " 0.0022962666116654873,\n",
       " 1781.065185546875,\n",
       " 0.6253748536109924,\n",
       " 0.4290815591812134,\n",
       " 0.006084173917770386,\n",
       " 0.422997385263443,\n",
       " 54.92243957519531,\n",
       " 0.5565457940101624,\n",
       " -0.0006202630465850234,\n",
       " -0.0004228467878419906,\n",
       " -0.0001974162587430328,\n",
       " 6710.26123046875,\n",
       " 0.6630493402481079,\n",
       " 0.0002834243350662291,\n",
       " 3.521514008753002e-05,\n",
       " 0.0002482091949786991,\n",
       " 6416.06103515625,\n",
       " 0.39350804686546326,\n",
       " 0.35332489013671875,\n",
       " -0.0011123418807983398,\n",
       " 0.3544372320175171,\n",
       " 45.2255859375,\n",
       " 0.8323776125907898,\n",
       " 0.0008166403276845813,\n",
       " 0.0008698796154931188,\n",
       " -5.323930599843152e-05,\n",
       " 7366.67138671875,\n",
       " 1.5197118520736694,\n",
       " 0.969868004322052,\n",
       " -0.019374489784240723,\n",
       " 0.9892424941062927,\n",
       " 496.5724182128906,\n",
       " 0.6527848243713379,\n",
       " -0.0006573495920747519,\n",
       " -0.0006929236697033048,\n",
       " 3.5574077628552914e-05,\n",
       " 8060.0478515625,\n",
       " 1.6567747592926025,\n",
       " 1.215706467628479,\n",
       " -0.00471651554107666,\n",
       " 1.2204229831695557,\n",
       " 155.6104278564453,\n",
       " 0.8892714381217957,\n",
       " 0.0006889096694067121,\n",
       " 0.0002493272768333554,\n",
       " 0.00043958239257335663,\n",
       " 6995.9091796875,\n",
       " 0.2868988811969757,\n",
       " 0.1451462358236313,\n",
       " 0.011140689253807068,\n",
       " 0.13400554656982422,\n",
       " 18.578718185424805,\n",
       " 0.4626751244068146,\n",
       " 0.000878011982422322,\n",
       " -6.919685984030366e-05,\n",
       " 0.0009472088422626257,\n",
       " 5768.662109375,\n",
       " 0.47955775260925293,\n",
       " -4.634716606233269e-05,\n",
       " -0.0004519405192695558,\n",
       " 0.0004055933386553079,\n",
       " 4762.63720703125,\n",
       " 0.6178663969039917,\n",
       " 0.524685263633728,\n",
       " 0.0005434751510620117,\n",
       " 0.524141788482666,\n",
       " 67.15971374511719,\n",
       " 0.5029813051223755,\n",
       " 0.00027684701490215957,\n",
       " -2.8848153306171298e-05,\n",
       " 0.00030569516820833087,\n",
       " 6162.2373046875,\n",
       " 0.4846237599849701,\n",
       " -4.251906284480356e-05,\n",
       " -0.00020023896649945527,\n",
       " 0.0001577199000166729,\n",
       " 5200.7646484375,\n",
       " 0.5348517298698425,\n",
       " 0.47265738248825073,\n",
       " 0.0012760460376739502,\n",
       " 0.4713813364505768,\n",
       " 60.500144958496094,\n",
       " 0.6754872798919678,\n",
       " 0.00047130673192441463,\n",
       " 0.0001159363891929388,\n",
       " 0.00035537034273147583,\n",
       " 6341.70947265625,\n",
       " 0.515224814414978,\n",
       " 2.978711563628167e-05,\n",
       " 6.105416105128825e-06,\n",
       " 2.3681699531152844e-05,\n",
       " 5679.88134765625,\n",
       " 0.49631527066230774,\n",
       " 0.42382848262786865,\n",
       " -0.0009531080722808838,\n",
       " 0.42478159070014954,\n",
       " 54.25004577636719,\n",
       " 1.367317795753479,\n",
       " 0.0014955186052247882,\n",
       " 0.000324331340380013,\n",
       " 0.0011711872648447752,\n",
       " 2789.185302734375,\n",
       " 1.1703664064407349,\n",
       " 0.0016736425459384918,\n",
       " 0.00013416726142168045,\n",
       " 0.0015394752845168114,\n",
       " 2432.111328125,\n",
       " 1.0059678554534912,\n",
       " 0.0009308759472332895,\n",
       " -0.0001238881959579885,\n",
       " 0.001054764143191278,\n",
       " 8470.4375,\n",
       " 0.6412060856819153,\n",
       " 0.0008184998296201229,\n",
       " 0.0007068530539982021,\n",
       " 0.00011164677562192082,\n",
       " 1863.32958984375,\n",
       " 0.6842422485351562,\n",
       " 0.44696080684661865,\n",
       " 0.00825655460357666,\n",
       " 0.438704252243042,\n",
       " 57.21098327636719,\n",
       " 0.6384874582290649,\n",
       " 0.0019685334991663694,\n",
       " 0.00047971075400710106,\n",
       " 0.0014888227451592684,\n",
       " 5612.91845703125,\n",
       " 0.6681577563285828,\n",
       " -0.0008006750140339136,\n",
       " 0.0004560472443699837,\n",
       " -0.0012567222584038973,\n",
       " 5298.603515625,\n",
       " 0.4403635561466217,\n",
       " 0.3810253441333771,\n",
       " 0.0002065598964691162,\n",
       " 0.38081878423690796,\n",
       " 48.771244049072266,\n",
       " 1.0408079624176025,\n",
       " -6.55279727652669e-05,\n",
       " 0.00016517005860805511,\n",
       " -0.000230698031373322,\n",
       " 7590.99365234375,\n",
       " 1.2036744356155396,\n",
       " 0.8881253004074097,\n",
       " 0.019265949726104736,\n",
       " 0.8688593506813049,\n",
       " 454.72015380859375,\n",
       " 0.6420785784721375,\n",
       " 0.0008854572661221027,\n",
       " -0.00010507344268262386,\n",
       " 0.0009905307088047266,\n",
       " 8174.56494140625,\n",
       " 1.9734467267990112,\n",
       " 1.3002835512161255,\n",
       " 0.005942106246948242,\n",
       " 1.2943414449691772,\n",
       " 166.43629455566406,\n",
       " 1.611202597618103,\n",
       " 0.00015080845332704484,\n",
       " 0.00015224666276481003,\n",
       " -1.4382039807969704e-06,\n",
       " 6525.109375,\n",
       " 0.29342323541641235,\n",
       " 0.09149612486362457,\n",
       " 0.013104692101478577,\n",
       " 0.078391432762146,\n",
       " 11.711503982543945,\n",
       " 0.4538153111934662,\n",
       " 0.00031034284620545805,\n",
       " 0.00034747220342978835,\n",
       " -3.7129371776245534e-05,\n",
       " 5092.43017578125,\n",
       " 0.6771497130393982,\n",
       " -0.000603610067628324,\n",
       " -0.00015114119742065668,\n",
       " -0.00045246887020766735,\n",
       " 3961.494140625,\n",
       " 0.6442203521728516,\n",
       " 0.5515297651290894,\n",
       " 0.00046074390411376953,\n",
       " 0.5510690212249756,\n",
       " 70.59580993652344,\n",
       " 0.6440694332122803,\n",
       " 0.000836104794871062,\n",
       " 0.0001869569532573223,\n",
       " 0.0006491478416137397,\n",
       " 5731.0,\n",
       " 0.463685542345047,\n",
       " -0.0001118220025091432,\n",
       " 0.000147559359902516,\n",
       " -0.0002593813696876168,\n",
       " 4758.51025390625,\n",
       " 0.6126284599304199,\n",
       " 0.4997358024120331,\n",
       " 0.004189044237136841,\n",
       " 0.49554675817489624,\n",
       " 63.966182708740234,\n",
       " 0.5468379855155945,\n",
       " 0.001495551667176187,\n",
       " -0.00022352009546011686,\n",
       " 0.001719071762636304,\n",
       " 5962.30712890625,\n",
       " 0.6388063430786133,\n",
       " -0.0007500218343921006,\n",
       " -0.0003239725192543119,\n",
       " -0.00042604931513778865,\n",
       " 5242.98388671875,\n",
       " 0.5175870656967163,\n",
       " 0.4449952244758606,\n",
       " -0.00010630488395690918,\n",
       " 0.4451015293598175,\n",
       " 56.959388732910156,\n",
       " 0.5432180166244507,\n",
       " -5.851109744980931e-05,\n",
       " -0.0005603893077932298,\n",
       " 0.0005018782103434205,\n",
       " 1608.8011474609375,\n",
       " 0.5413668155670166,\n",
       " 0.0017009754665195942,\n",
       " -0.00043755117803812027,\n",
       " 0.0021385266445577145,\n",
       " 1419.065185546875,\n",
       " 0.7958290576934814,\n",
       " 0.0001883727964013815,\n",
       " 0.0004426897212397307,\n",
       " -0.0002543169248383492,\n",
       " 8221.3251953125,\n",
       " 0.5019201040267944,\n",
       " 0.0008100143168121576,\n",
       " -0.0007225095760077238,\n",
       " 0.0015325238928198814,\n",
       " 1715.04150390625,\n",
       " 0.605785608291626,\n",
       " 0.376839280128479,\n",
       " 0.006853818893432617,\n",
       " 0.3699854612350464,\n",
       " 48.23542785644531,\n",
       " 0.9831079244613647,\n",
       " 0.004097649361938238,\n",
       " 0.00047678640112280846,\n",
       " 0.0036208629608154297,\n",
       " 7021.9150390625,\n",
       " 0.7485809326171875,\n",
       " -0.0007797582074999809,\n",
       " -0.00026410806458443403,\n",
       " -0.0005156501429155469,\n",
       " 6771.662109375,\n",
       " 0.4440699517726898,\n",
       " 0.3837721347808838,\n",
       " 0.0013165473937988281,\n",
       " 0.38245558738708496,\n",
       " 49.122833251953125,\n",
       " 0.7721041440963745,\n",
       " 0.00039363402174785733,\n",
       " 9.698115172795951e-05,\n",
       " 0.0002966528700198978,\n",
       " 7363.59423828125,\n",
       " 2.1891300678253174,\n",
       " 0.7703859806060791,\n",
       " 0.013621151447296143,\n",
       " 0.756764829158783,\n",
       " 394.4376220703125,\n",
       " 0.6088111400604248,\n",
       " -2.6010748115368187e-05,\n",
       " 0.0004493109881877899,\n",
       " -0.00047532175085507333,\n",
       " 7618.7119140625,\n",
       " 1.5334107875823975,\n",
       " 1.1630083322525024,\n",
       " 0.005777955055236816,\n",
       " 1.1572303771972656,\n",
       " 148.8650665283203,\n",
       " 0.8319947123527527,\n",
       " -0.0010328484931960702,\n",
       " 0.0004718477139249444,\n",
       " -0.0015046962071210146,\n",
       " 7378.30419921875,\n",
       " 0.27194586396217346,\n",
       " 0.12108638137578964,\n",
       " 0.007225021719932556,\n",
       " 0.11386135965585709,\n",
       " 15.499056816101074,\n",
       " 0.5014929175376892,\n",
       " 0.0025078365579247475,\n",
       " 0.0004333113320171833,\n",
       " 0.002074525225907564,\n",
       " 6034.0546875,\n",
       " 0.5864751935005188,\n",
       " -0.001151805161498487,\n",
       " 0.00019582768436521292,\n",
       " -0.0013476328458637,\n",
       " 5308.1484375,\n",
       " 0.5558444857597351,\n",
       " 0.47663363814353943,\n",
       " 0.004975110292434692,\n",
       " 0.47165852785110474,\n",
       " 61.00910568237305,\n",
       " 0.5496241450309753,\n",
       " 0.00319768488407135,\n",
       " -0.00043436535634100437,\n",
       " 0.0036320502404123545,\n",
       " 6453.2763671875,\n",
       " 0.7227967977523804,\n",
       " -0.0008713314891792834,\n",
       " -3.590103005990386e-05,\n",
       " -0.0008354304591193795,\n",
       " 5769.9658203125,\n",
       " 0.5284978747367859,\n",
       " 0.4375346302986145,\n",
       " 0.006280720233917236,\n",
       " 0.43125391006469727,\n",
       " 56.004432678222656,\n",
       " 0.5430523157119751,\n",
       " 0.0029390519484877586,\n",
       " -0.0006169218104332685,\n",
       " 0.003555973758921027,\n",
       " 6721.42529296875,\n",
       " 0.6687162518501282,\n",
       " -0.0014614830724895,\n",
       " 0.00013370439410209656,\n",
       " -0.0015951874665915966,\n",
       " 6259.7548828125,\n",
       " 0.542896032333374,\n",
       " 0.4190305769443512,\n",
       " 0.0062628090381622314,\n",
       " 0.41276776790618896,\n",
       " 53.63591384887695,\n",
       " 0.5603978633880615,\n",
       " 0.000710686668753624,\n",
       " 0.0003258095239289105,\n",
       " 0.00038487714482471347,\n",
       " 1517.2357177734375,\n",
       " 0.643068790435791,\n",
       " -0.00035315926652401686,\n",
       " -0.0006732628680765629,\n",
       " 0.0003201036306563765,\n",
       " 1607.486572265625,\n",
       " 0.9254973530769348,\n",
       " 0.0013373509282246232,\n",
       " 0.0005794224562123418,\n",
       " 0.0007579284720122814,\n",
       " 7744.25,\n",
       " 0.6035122871398926,\n",
       " 0.0031733522191643715,\n",
       " 0.0017362020444124937,\n",
       " 0.0014371501747518778,\n",
       " 1828.55322265625,\n",
       " 0.7275775671005249,\n",
       " 0.47355741262435913,\n",
       " 0.004495143890380859,\n",
       " 0.46906226873397827,\n",
       " 60.61534881591797,\n",
       " 0.5559234023094177,\n",
       " -0.0004459649499040097,\n",
       " -0.000699160504154861,\n",
       " 0.00025319558335468173,\n",
       " 5723.09423828125,\n",
       " 0.6144284009933472,\n",
       " 0.0002009403397096321,\n",
       " -0.0002200679009547457,\n",
       " 0.0004210082406643778,\n",
       " 5458.06396484375,\n",
       " 0.46443817019462585,\n",
       " 0.39457404613494873,\n",
       " -0.0006454586982727051,\n",
       " 0.39521950483322144,\n",
       " 50.50547790527344,\n",
       " 0.8579279184341431,\n",
       " 0.000561885884962976,\n",
       " -0.00037040881579741836,\n",
       " 0.0009322947007603943,\n",
       " 7035.033203125,\n",
       " 1.486309289932251,\n",
       " 1.014913558959961,\n",
       " -0.006594538688659668,\n",
       " 1.0215080976486206,\n",
       " 519.6357421875,\n",
       " 0.665225088596344,\n",
       " 0.0004226536548230797,\n",
       " -0.0004833189013879746,\n",
       " 0.0009059725562110543,\n",
       " 7720.73291015625,\n",
       " 1.5706409215927124,\n",
       " 1.0895313024520874,\n",
       " -0.02173173427581787,\n",
       " 1.1112630367279053,\n",
       " 139.4600067138672,\n",
       " 0.6493967175483704,\n",
       " -0.0004443309735506773,\n",
       " -0.0003375258529558778,\n",
       " -0.00010680512787075713,\n",
       " 6971.0322265625,\n",
       " 0.2790590822696686,\n",
       " 0.12729191780090332,\n",
       " 0.007402375340461731,\n",
       " 0.11988954246044159,\n",
       " 16.293365478515625,\n",
       " 0.4014330506324768,\n",
       " -0.0016273861983790994,\n",
       " 0.0001598572125658393,\n",
       " -0.0017872434109449387,\n",
       " 4873.478515625,\n",
       " 0.5483062863349915,\n",
       " -0.0001452076539862901,\n",
       " -0.00010436238517286256,\n",
       " -4.084526881342754e-05,\n",
       " 3606.591796875,\n",
       " 0.6910613775253296,\n",
       " 0.553642213344574,\n",
       " 0.0017413496971130371,\n",
       " 0.5519008636474609,\n",
       " 70.86620330810547,\n",
       " 0.4227805733680725,\n",
       " -0.0008901895489543676,\n",
       " -0.00027322384994477034,\n",
       " -0.0006169656990095973,\n",
       " 5196.02001953125,\n",
       " 0.6609792113304138,\n",
       " 7.21509859431535e-05,\n",
       " -6.682178354822099e-05,\n",
       " 0.0001389727694913745,\n",
       " 3967.6904296875,\n",
       " 0.6156893968582153,\n",
       " 0.5596414804458618,\n",
       " -1.6033649444580078e-05,\n",
       " 0.5596575140953064,\n",
       " 71.63410949707031,\n",
       " 0.5136657953262329,\n",
       " -0.0006176512688398361,\n",
       " 9.14823031052947e-05,\n",
       " -0.0007091335719451308,\n",
       " 5542.2060546875,\n",
       " 0.5829421877861023,\n",
       " 0.00032867069239728153,\n",
       " 0.0003130942059215158,\n",
       " 1.557647919980809e-05,\n",
       " 4746.759765625,\n",
       " 0.5880418419837952,\n",
       " 0.5067038536071777,\n",
       " 0.0027930736541748047,\n",
       " 0.5039107799530029,\n",
       " 64.85809326171875,\n",
       " 0.9750508666038513,\n",
       " 0.00012726872228085995,\n",
       " 0.0002622951869852841,\n",
       " -0.00013502647925633937,\n",
       " 2152.0625,\n",
       " 0.9408717155456543,\n",
       " 0.0024553374387323856,\n",
       " 0.001177408266812563,\n",
       " 0.0012779291719198227,\n",
       " 2005.1954345703125,\n",
       " 0.8333939909934998,\n",
       " 0.0012121223844587803,\n",
       " 0.0006252703606151044,\n",
       " 0.0005868520238436759,\n",
       " 8143.9833984375,\n",
       " 0.5671981573104858,\n",
       " -0.00037638447247445583,\n",
       " -0.00031157059129327536,\n",
       " -6.481388118118048e-05,\n",
       " 1895.5833740234375,\n",
       " 0.748998761177063,\n",
       " 0.4496777653694153,\n",
       " 0.0030849874019622803,\n",
       " 0.446592777967453,\n",
       " 57.558753967285156,\n",
       " 0.781336784362793,\n",
       " -0.00035577977541834116,\n",
       " -0.0004789757076650858,\n",
       " 0.00012319593224674463,\n",
       " 5827.052734375,\n",
       " 0.7183477282524109,\n",
       " 6.653503805864602e-05,\n",
       " 7.503828965127468e-05,\n",
       " -8.50325250212336e-06,\n",
       " 5538.6220703125,\n",
       " 0.42852333188056946,\n",
       " 0.37912166118621826,\n",
       " -0.0028818845748901367,\n",
       " 0.3820035457611084,\n",
       " 48.52757263183594,\n",
       " 0.7900039553642273,\n",
       " -0.00025420778547413647,\n",
       " 0.0005126390606164932,\n",
       " -0.0007668468169867992,\n",
       " 7308.54541015625,\n",
       " 1.2607803344726562,\n",
       " 0.9806129336357117,\n",
       " -0.008442223072052002,\n",
       " 0.9890551567077637,\n",
       " 502.0738220214844,\n",
       " 0.7060534358024597,\n",
       " 0.0007661550189368427,\n",
       " 0.0002772273728623986,\n",
       " 0.0004889276460744441,\n",
       " 7942.32080078125,\n",
       " 1.6597923040390015,\n",
       " 1.241525411605835,\n",
       " 0.010861396789550781,\n",
       " 1.2306640148162842,\n",
       " 158.91525268554688,\n",
       " 0.7901859879493713,\n",
       " 0.0003273245529271662,\n",
       " 0.00016253643843811005,\n",
       " 0.00016478811448905617,\n",
       " 6644.55126953125,\n",
       " 0.2631291449069977,\n",
       " 0.12857821583747864,\n",
       " 0.005806535482406616,\n",
       " 0.12277168035507202,\n",
       " 16.458011627197266,\n",
       " 0.43844351172447205,\n",
       " 0.00018302607350051403,\n",
       " 0.0002959559205919504,\n",
       " -0.00011292986164335161,\n",
       " 5103.056640625,\n",
       " 0.5667625665664673,\n",
       " 0.0001990937744267285,\n",
       " -0.0003538639866746962,\n",
       " 0.0005529577611014247,\n",
       " 4117.384765625,\n",
       " 0.6526384353637695,\n",
       " 0.5360267162322998,\n",
       " 0.005397915840148926,\n",
       " 0.5306288003921509,\n",
       " 68.61141967773438,\n",
       " 0.7053526639938354,\n",
       " 0.00022389262448996305,\n",
       " 0.00012436354882083833,\n",
       " 9.952907566912472e-05,\n",
       " 5874.65283203125,\n",
       " 0.5960510969161987,\n",
       " 0.00026112893829122186,\n",
       " -0.00031346658943220973,\n",
       " 0.0005745955277234316,\n",
       " 5062.5888671875,\n",
       " 0.5394552946090698,\n",
       " 0.47198161482810974,\n",
       " 0.0016264915466308594,\n",
       " 0.4703551232814789,\n",
       " 60.41364669799805,\n",
       " 0.566490113735199,\n",
       " 0.0003735013015102595,\n",
       " 1.501553924754262e-06,\n",
       " 0.00037199974758550525,\n",
       " 6001.7646484375,\n",
       " 0.8423940539360046,\n",
       " -0.0005191626260057092,\n",
       " 0.00015623157378286123,\n",
       " -0.0006753941997885704,\n",
       " 5432.60888671875,\n",
       " 0.5119730830192566,\n",
       " 0.4403146505355835,\n",
       " -0.0003368556499481201,\n",
       " 0.4406515061855316,\n",
       " 56.36027526855469,\n",
       " 1.0043154954910278,\n",
       " 0.0035330026876181364,\n",
       " 0.0012588135432451963,\n",
       " 0.00227418914437294,\n",
       " 2253.02783203125,\n",
       " 0.7558817863464355,\n",
       " -0.00016924337251111865,\n",
       " -1.6546982806175947e-06,\n",
       " -0.00016758867423050106,\n",
       " 1855.946044921875,\n",
       " 0.9418743252754211,\n",
       " 0.0003312177141197026,\n",
       " -0.00020315690198913217,\n",
       " 0.0005343746161088347,\n",
       " 8051.736328125,\n",
       " 0.5699363350868225,\n",
       " -0.0010328786447644234,\n",
       " -0.00024363130796700716,\n",
       " -0.0007892473367974162,\n",
       " 1877.453369140625,\n",
       " 0.8161306977272034,\n",
       " 0.44117435812950134,\n",
       " 0.010602325201034546,\n",
       " 0.4305720329284668,\n",
       " 56.47031784057617,\n",
       " 0.6809409260749817,\n",
       " -0.0023361248895525932,\n",
       " 0.00023335497826337814,\n",
       " -0.0025694798678159714,\n",
       " 5855.5263671875,\n",
       " 0.7363048791885376,\n",
       " 1.1708179954439402e-05,\n",
       " 0.00015704432735219598,\n",
       " -0.00014533614739775658,\n",
       " 5592.1748046875,\n",
       " 0.42161068320274353,\n",
       " 0.3814529478549957,\n",
       " -0.001695007085800171,\n",
       " 0.3831479549407959,\n",
       " 48.82597732543945,\n",
       " 0.9243902564048767,\n",
       " 0.0005203257314860821,\n",
       " 8.840573718771338e-05,\n",
       " 0.0004319199942983687,\n",
       " 7509.16064453125,\n",
       " 1.2804163694381714,\n",
       " 0.9753929972648621,\n",
       " -0.021107196807861328,\n",
       " 0.9965001940727234,\n",
       " 499.4012145996094,\n",
       " 0.648393988609314,\n",
       " 4.227239696774632e-05,\n",
       " 0.00046118791215121746,\n",
       " -0.0004189155006315559,\n",
       " 7869.3720703125,\n",
       " 1.622908353805542,\n",
       " 1.1474621295928955,\n",
       " 0.011924505233764648,\n",
       " 1.1355376243591309,\n",
       " 146.87515258789062,\n",
       " 0.9714645743370056,\n",
       " -7.20893222023733e-05,\n",
       " -0.00012465682812035084,\n",
       " 5.256750591797754e-05,\n",
       " 6430.248046875,\n",
       " 0.29002636671066284,\n",
       " 0.0957990512251854,\n",
       " 0.009425684809684753,\n",
       " 0.08637336641550064,\n",
       " 12.26227855682373,\n",
       " 0.3927824795246124,\n",
       " 0.001342335599474609,\n",
       " -0.00012551236432045698,\n",
       " 0.0014678479637950659,\n",
       " 5012.98974609375,\n",
       " 0.5011895895004272,\n",
       " -0.00016269247862510383,\n",
       " -3.627769183367491e-05,\n",
       " -0.00012641478679142892,\n",
       " 4067.663818359375,\n",
       " 0.6318296790122986,\n",
       " 0.5340077877044678,\n",
       " 0.004407286643981934,\n",
       " 0.5296005010604858,\n",
       " 68.35299682617188,\n",
       " 0.5118805766105652,\n",
       " 0.0002575326943770051,\n",
       " -0.0005119664128869772,\n",
       " 0.0007694991072639823,\n",
       " 5679.1923828125,\n",
       " 0.8143597841262817,\n",
       " 0.00016909060650505126,\n",
       " 0.00011324664956191555,\n",
       " 5.584395694313571e-05,\n",
       " 5052.01025390625,\n",
       " 0.5372321605682373,\n",
       " 0.46493926644325256,\n",
       " 0.002024710178375244,\n",
       " 0.4629145562648773,\n",
       " 59.51222610473633,\n",
       " 0.6490640640258789,\n",
       " 0.00018665287643671036,\n",
       " -0.0002536380779929459,\n",
       " 0.00044029095442965627,\n",
       " 6055.935546875,\n",
       " 0.9888170957565308,\n",
       " 0.0002959774574264884,\n",
       " -5.6583317928016186e-05,\n",
       " 0.0003525607753545046,\n",
       " 5696.115234375,\n",
       " 0.5207465887069702,\n",
       " 0.43098482489585876,\n",
       " 0.0013917982578277588,\n",
       " 0.429593026638031,\n",
       " 55.16605758666992,\n",
       " 0.6455217599868774,\n",
       " 0.001987089868634939,\n",
       " 0.0010841465555131435,\n",
       " 0.0009029433131217957,\n",
       " 1813.6832275390625,\n",
       " 0.47059884667396545,\n",
       " 0.0005672844126820564,\n",
       " -0.000370451423805207,\n",
       " 0.0009377358364872634,\n",
       " 1329.895751953125,\n",
       " 0.9638286828994751,\n",
       " 0.0003388709737919271,\n",
       " 7.161195389926434e-05,\n",
       " 0.00026725901989266276,\n",
       " 8053.16015625,\n",
       " 0.6159427762031555,\n",
       " -0.0004080737999174744,\n",
       " -1.7265265341848135e-06,\n",
       " -0.0004063472733832896,\n",
       " 1933.7652587890625,\n",
       " 0.6333323121070862,\n",
       " 0.42627447843551636,\n",
       " 0.00270882248878479,\n",
       " 0.42356565594673157,\n",
       " 54.563133239746094,\n",
       " 0.6119592189788818,\n",
       " -0.0030413861386477947,\n",
       " 8.925609290599823e-05,\n",
       " -0.003130642231553793,\n",
       " 5434.3857421875,\n",
       " 0.7319828867912292,\n",
       " 0.0002773996093310416,\n",
       " 0.0001507526176283136,\n",
       " 0.00012664699170272797,\n",
       " 5395.3720703125,\n",
       " 0.5112396478652954,\n",
       " 0.4426330626010895,\n",
       " -0.0009250342845916748,\n",
       " 0.44355809688568115,\n",
       " 56.65703201293945,\n",
       " 1.4677395820617676,\n",
       " -0.0015967607032507658,\n",
       " -0.0006565623916685581,\n",
       " -0.0009401983115822077,\n",
       " 7469.0302734375,\n",
       " 1.033682107925415,\n",
       " 0.8610618114471436,\n",
       " -0.037915706634521484,\n",
       " 0.898977518081665,\n",
       " 440.8636474609375,\n",
       " 0.6630746722221375,\n",
       " -3.840389763354324e-05,\n",
       " 0.0008406979031860828,\n",
       " -0.0008791018044576049,\n",
       " 7618.20947265625,\n",
       " ...]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "265\n",
      "505\n",
      "265\n",
      "265\n",
      "505\n",
      "505\n",
      "265\n",
      "505\n",
      "505\n",
      "265\n",
      "505\n",
      "265\n",
      "265\n",
      "265\n",
      "265\n",
      "265\n",
      "505\n",
      "505\n",
      "265\n",
      "265\n",
      "265\n",
      "505\n",
      "265\n",
      "505\n",
      "505\n",
      "265\n",
      "505\n",
      "505\n",
      "265\n",
      "505\n",
      "505\n",
      "505\n",
      "505\n",
      "505\n",
      "265\n",
      "505\n",
      "265\n",
      "265\n",
      "505\n",
      "265\n",
      "265\n",
      "505\n",
      "265\n",
      "505\n",
      "505\n",
      "265\n",
      "505\n",
      "265\n",
      "265\n",
      "265\n",
      "505\n",
      "265\n",
      "505\n",
      "265\n",
      "505\n",
      "505\n",
      "265\n",
      "265\n",
      "505\n",
      "505\n",
      "505\n",
      "265\n",
      "265\n",
      "505\n",
      "265\n",
      "265\n",
      "505\n",
      "265\n",
      "505\n",
      "265\n",
      "265\n",
      "505\n",
      "505\n",
      "505\n",
      "505\n",
      "505\n",
      "265\n",
      "505\n",
      "265\n",
      "265\n",
      "2790\n",
      "2790\n",
      "2790\n",
      "2790\n",
      "2790\n",
      "2790\n",
      "2790\n",
      "2790\n",
      "2790\n",
      "2790\n",
      "2790\n",
      "2790\n",
      "2790\n",
      "2790\n",
      "2790\n",
      "2790\n",
      "2790\n",
      "2790\n",
      "2790\n",
      "2790\n",
      "2790\n",
      "2790\n",
      "2790\n",
      "2790\n",
      "2790\n",
      "2790\n",
      "2790\n",
      "2790\n",
      "2790\n",
      "2790\n",
      "2790\n",
      "2790\n",
      "2790\n",
      "2790\n",
      "2790\n",
      "2790\n",
      "2790\n",
      "2790\n",
      "2790\n",
      "2790\n"
     ]
    }
   ],
   "source": [
    "for k, v in X.items():\n",
    "    for v_ in v:\n",
    "        print(len(v_))\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SSD: (cen array([31,  3, 32, 37, 39, 29, 14,  0, 22, 35]), acc array([31, 37,  3, 42, 25, 39, 32, 45, 29, 19]))\n",
    "DETR: (array([ 80,  55, 134,  52, 285, 321, 267, 320, 299, 305]), array([ 80,  55,  46,  52, 113, 320, 305, 134, 290, 215]))\n",
    "RCNN: (array([147,  12,  18, 164, 161, 169, 114,  72,  45,  53]), array([164, 169, 161, 147,  12,  18, 114, 127, 137, 140]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_FILEDIR = '/scratch/jialin/nlp-question-answering-aug2023/extracted_features'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (80,) + inhomogeneous part.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mfor\u001b[39;00m k, v \u001b[39min\u001b[39;00m X\u001b[39m.\u001b[39mitems():\n\u001b[0;32m----> 2\u001b[0m     np\u001b[39m.\u001b[39;49msave(os\u001b[39m.\u001b[39;49mpath\u001b[39m.\u001b[39;49mjoin(OUTPUT_FILEDIR, \u001b[39mf\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39m{\u001b[39;49;00mk\u001b[39m}\u001b[39;49;00m\u001b[39m_X_more.npy\u001b[39;49m\u001b[39m'\u001b[39;49m), v)\n\u001b[1;32m      3\u001b[0m     np\u001b[39m.\u001b[39msave(os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(OUTPUT_FILEDIR, \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mk\u001b[39m}\u001b[39;00m\u001b[39m_y_more.npy\u001b[39m\u001b[39m'\u001b[39m), y[k])\n\u001b[1;32m      4\u001b[0m     \u001b[39m# np.save(os.path.join(OUTPUT_FILEDIR, f'{k}_X_len.npy'), X_len[k])\u001b[39;00m\n",
      "File \u001b[0;32m/scratch/jialin/anaconda3/envs/nlp-qa-aug2023/lib/python3.11/site-packages/numpy/lib/npyio.py:545\u001b[0m, in \u001b[0;36msave\u001b[0;34m(file, arr, allow_pickle, fix_imports)\u001b[0m\n\u001b[1;32m    542\u001b[0m     file_ctx \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39m(file, \u001b[39m\"\u001b[39m\u001b[39mwb\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    544\u001b[0m \u001b[39mwith\u001b[39;00m file_ctx \u001b[39mas\u001b[39;00m fid:\n\u001b[0;32m--> 545\u001b[0m     arr \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49masanyarray(arr)\n\u001b[1;32m    546\u001b[0m     \u001b[39mformat\u001b[39m\u001b[39m.\u001b[39mwrite_array(fid, arr, allow_pickle\u001b[39m=\u001b[39mallow_pickle,\n\u001b[1;32m    547\u001b[0m                        pickle_kwargs\u001b[39m=\u001b[39m\u001b[39mdict\u001b[39m(fix_imports\u001b[39m=\u001b[39mfix_imports))\n",
      "\u001b[0;31mValueError\u001b[0m: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (80,) + inhomogeneous part."
     ]
    }
   ],
   "source": [
    "for k, v in X.items():\n",
    "    np.save(os.path.join(OUTPUT_FILEDIR, f'{k}_X_more.npy'), v)\n",
    "    np.save(os.path.join(OUTPUT_FILEDIR, f'{k}_y_more.npy'), y[k])\n",
    "    # np.save(os.path.join(OUTPUT_FILEDIR, f'{k}_X_len.npy'), X_len[k])\n",
    "    print(k, len(v[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:07<00:00,  2.49s/it]\n"
     ]
    }
   ],
   "source": [
    "X_len = {}\n",
    "for model_num in tqdm([0, 1, 3]):\n",
    "    model_id = num_to_model_id(model_num)\n",
    "    model_filepath = os.path.join(MODEL_FILEDIR, model_id, 'model.pt')\n",
    "    model, _, model_class = model_utils.load_model(model_filepath)\n",
    "    model = model.to(device)\n",
    "    if 'Detr' not in model_class:\n",
    "        model_backbone = model.backbone\n",
    "    else:\n",
    "        model_backbone = model\n",
    "\n",
    "    fe_len, idx_list = [], [(0, 1e8)]\n",
    "    # if 'SSD' in model_class:\n",
    "        # idx_list = [(2, 5)]\n",
    "    # elif 'RCNN' in model_class:\n",
    "        # idx_list = [(2, 3)]\n",
    "    # else:\n",
    "        # idx_list = [(45, 45), (69, 69), (79, 79), (89, 89)]\n",
    "            \n",
    "    for idx_lo, idx_hi in idx_list:\n",
    "        fe_len = _get_eigen_vals(model_backbone, idx_lo, idx_hi)[1]\n",
    "    X_len[model_class] = fe_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "key = 'FasterRCNN'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_range = [0] + X_len[key]\n",
    "# weight_range = list(range(0, len(X[key][0])+1, 8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "cens, accs = [], []\n",
    "clf = GradientBoostingClassifier(learning_rate=0.015, n_estimators=900, max_depth=3, max_features= 120, min_samples_leaf= 6, min_samples_split= 24)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import log_loss\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "def bootstrap_performance(X, y, clf, n=10, test_size=.2, ret_fe_rank = False):\n",
    "    all_cross_entropy, all_accuracy = [], []\n",
    "    if ret_fe_rank:\n",
    "        all_fe_importance = []\n",
    "    \n",
    "    for i in range(n):\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=i)\n",
    "        \n",
    "        # clf.set_params(random_state=i)            \n",
    "        clf.fit(X_train, y_train)\n",
    "        \n",
    "        all_cross_entropy.append(log_loss(y_test, clf.predict_proba(X_test), labels=[0, 1]))\n",
    "        all_accuracy.append(clf.score(X_test, y_test))\n",
    "        if ret_fe_rank:\n",
    "            all_fe_importance.append(clf.feature_importances_)\n",
    "    if ret_fe_rank:\n",
    "        return all_cross_entropy, all_accuracy, all_fe_importance\n",
    "    return all_cross_entropy, all_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = GradientBoostingClassifier(n_estimators=1100, learning_rate=0.00225, max_depth=8, min_samples_split=17, subsample=.66, min_samples_leaf=4, max_features=230)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = {}, {}\n",
    "for model_class in ['BasicFCModel', 'SimplifiedRLStarter']:\n",
    "    X[model_class] = np.load(os.path.join(OUTPUT_FILEDIR, f'{model_class}_X.npy'))\n",
    "    y[model_class] = np.load(os.path.join(OUTPUT_FILEDIR, f'{model_class}_y.npy'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "fe_inds = json.load(open('/scratch/jialin/rl-lavaworld-jul2023/extracted_features/fe_ind2.json', 'r'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BasicFCModel 0.07416443662587176 0.9941463414634145\n",
      "SimplifiedRLStarter 0.07242138362473735 0.9936363636363638\n"
     ]
    }
   ],
   "source": [
    "fe_importance = {}\n",
    "for k, v in X.items():\n",
    "    cen, acc, fe_rank = bootstrap_performance(v, y[k], clf, n=50, test_size=.2, ret_fe_rank=True)\n",
    "    print(k, np.mean(cen), np.mean(acc))\n",
    "    fe_importance[k] = fe_rank\n",
    "    # print(np.argsort(cen)[:10], np.asarray(cen)[np.argsort(cen)[:10]])\n",
    "    # print(np.argsort(acc)[::-1][:10], np.asarray(acc)[np.argsort(acc)[::-1][:10]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k, v in X.items():\n",
    "    # v_ind = v[:, fe_inds[k]]\n",
    "    # print(v_ind.shape)\n",
    "    joblib.dump(clf.fit(v, y[k]), os.path.join(OUTPUT_FILEDIR, f'{k}_clf3.joblib'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BasicFCModel 0.1160307642356217 0.9683333333333334\n",
    "SimplifiedRLStarter 0.11422876380716901 0.9758333333333333"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAmIAAAGzCAYAAACM3HvxAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAABemUlEQVR4nO3dd1gUx/8H8PeBcEcRRJqACKhYsKCiIhp7AWti7JpQrDEaWxITY0FjlGissWsUTew9Go1GUWNU7MFeEcVCEQtNBT3m94c/9ut5hx5yuATer+e55+HmZndml7ndz83OziqEEAJERERE9N4ZyV0BIiIioqKKgRgRERGRTBiIEREREcmEgRgRERGRTBiIEREREcmEgRgRERGRTBiIEREREcmEgRgRERGRTBiIEREREcmEgRgVSO7u7ggODpa7Gjo1adIEVatWNeg6X7x4gZEjR8LV1RVGRkb46KOPDLr+gm758uVQKBS4efOmwdZ54sQJ1K9fHxYWFlAoFIiKijLYuqnguHnzJhQKBZYvX/7eyw4ODoa7u/t7L5cK9jkit/ItEJs/fz4UCgV8fX3zq4hCLSEhAV999RUqVaoEc3NzWFhYwMfHBz/88AMeP34sd/XIwJYtW4affvoJnTt3xooVKzB8+HC5q/Sf9vz5c3Tp0gUPHz7EzJkz8dtvv8HNzQ3z589/7ydsd3d3KBQKna9nz57lS5mTJ0/G1q1b82XdeZWZmYnZs2ejZs2asLKyQokSJVClShX0798fly9flrt6Wu7du4fx48cXyUD+wIEDUCgU2Lhxo9xVybXg4OAcv3evvgwVzK1evRqzZs16p2WLGaQGOqxatQru7u44fvw4rl+/jvLly+dXUYXOiRMn0KZNG6SlpeGTTz6Bj48PAODkyZP48ccfcfDgQfz1118y1zJ/XblyBUZGRafDdt++fXBxccHMmTPlrkqhEB0djVu3bmHJkiXo27evlD5//nzY2dm991/SNWrUwJdffqmVbmpqmi/lTZ48GZ07dy6QPaudOnXCn3/+iR49eqBfv354/vw5Ll++jD/++AP169dHpUqV5K6ihnv37mHChAlwd3dHjRo1ND5bsmQJsrKy5KkYvdGAAQPQokUL6X1MTAzGjRuH/v37o2HDhlJ6uXLlDFLe6tWrcf78eQwbNizXy+ZLIBYTE4MjR45g8+bNGDBgAFatWoXQ0ND8KCrP0tPTYWFhIXc1JI8fP0bHjh1hbGyMf//9V+ugNGnSJCxZskSm2uUvIQSePXsGMzMzKJVKuavzXiUmJqJEiRIGW19WVhYyMzOhUqkMts7/ksTERAAw6D7NyYsXL5CVlfXGoMrFxQWffPJJvtclPxmiTZ04cQJ//PEHJk2ahO+++07js7lz5/7nevtNTEzkrgLlwM/PD35+ftL7kydPYty4cfDz8ytw38V86XJYtWoVbGxs0LZtW3Tu3BmrVq3Sme/x48cYPnw43N3doVQqUbp0aQQGBiIpKUnK8+zZM4wfPx4VKlSASqWCk5MTPv74Y0RHRwP4X9fpgQMHNNata9xAcHAwLC0tER0djTZt2qB48eLo1asXAOCff/5Bly5dUKZMGSiVSri6umL48OF4+vSpVr0vX76Mrl27wt7eHmZmZqhYsSJGjx4NANi/fz8UCgW2bNmitdzq1auhUCgQGRmZ475btGgR7t69ixkzZuj8Zejo6IgxY8ZopM2fPx9VqlSBUqmEs7MzBg0apHVAyx7XdPbsWTRu3Bjm5uYoX7681OX8999/w9fXV9qevXv3aiw/fvx4KBQKadutrKxga2uLoUOHal1eCQ8PR7NmzeDg4AClUgkvLy8sWLBAa1vc3d3Rrl077N69G7Vr14aZmRkWLVokffZqr8Xz588xYcIEeHp6QqVSwdbWFh988AH27Nmjsc59+/ahYcOGsLCwQIkSJfDhhx/i0qVLOrfl+vXrCA4ORokSJWBtbY2QkBA8efJEx39Ft1OnTqF+/fowMzODh4cHFi5cqJUnIyMDoaGhKF++vNSuRo4ciYyMDAD/a6f79+/HhQsXpO7y7Pacnp6OL7/8Eq6urlAqlahYsSKmTZsGIYRGOQqFAoMHD8aqVauktrBr1y4AwN27d9G7d284OjpCqVSiSpUqWLZsmV7bmNv/5aFDh1C3bl2oVCqULVsWv/76q1beCxcuoFmzZjAzM0Pp0qXxww8/6N2rcPbsWQQHB6Ns2bJQqVQoVaoUevfujQcPHkh5goOD0bhxYwBAly5doFAo0KRJE7i7u+PChQv4+++/pf3cpEkTabnHjx9j2LBh0r4uX748pkyZolG37P/XtGnTMGvWLJQrVw5KpRIXL17Uq/450adsAJg2bRrq168PW1tbmJmZwcfHR+uykUKhQHp6OlasWKF1+SWnMU3Z34nX12PoNpV93G7QoIHWZ8bGxrC1tdVIy0vbvXz5Mjp37oySJUtCpVKhdu3a2LZtm1a+N52HDhw4gDp16gAAQkJCpP2ZfV7RtT9z+53dunUrqlatKm1f9j5+m8TERPTp0weOjo5QqVTw9vbGihUrNPK82l4XL14stdc6dergxIkTepWjjxs3bqBLly4oWbIkzM3NUa9ePezYsUMr39vO54B+bdyQjh07hoCAAFhbW8Pc3ByNGzfG4cOHNfKkpqZi2LBhUhtxcHBAy5Ytcfr0aQAvz687duzArVu3pDaSm7GD+dIjtmrVKnz88ccwNTVFjx49sGDBApw4cUJq0ACQlpaGhg0b4tKlS+jduzdq1aqFpKQkbNu2DXfu3IGdnR3UajXatWuHiIgIdO/eHUOHDkVqair27NmD8+fPv1OX4osXL+Dv748PPvgA06ZNg7m5OQBgw4YNePLkCQYOHAhbW1scP34cc+bMwZ07d7BhwwZp+bNnz6Jhw4YwMTFB//794e7ujujoaGzfvh2TJk1CkyZN4OrqilWrVqFjx45a+6VcuXIaUfrrtm3bBjMzM3Tu3Fmv7Rk/fjwmTJiAFi1aYODAgbhy5Yq0vw8fPqzxi+3Ro0do164dunfvji5dumDBggXo3r07Vq1ahWHDhuGzzz5Dz549pbFKt2/fRvHixTXK69q1K9zd3REWFoajR4/i559/xqNHjzROuAsWLECVKlXQoUMHFCtWDNu3b8fnn3+OrKwsDBo0SGN9V65cQY8ePTBgwAD069cPFStWzHE7w8LC0LdvX9StWxcpKSk4efIkTp8+jZYtWwIA9u7di9atW6Ns2bIYP348nj59ijlz5qBBgwY4ffq01heja9eu8PDwQFhYGE6fPo1ffvkFDg4OmDJlylv3+6NHj9CmTRt07doVPXr0wPr16zFw4ECYmpqid+/eAF72IHTo0AGHDh1C//79UblyZZw7dw4zZ87E1atXsXXrVtjb2+O3337DpEmTkJaWhrCwMABA5cqVIYRAhw4dsH//fvTp0wc1atTA7t278fXXX+Pu3btalzH37duH9evXY/DgwbCzs4O7uzsSEhJQr1496aBvb2+PP//8E3369EFKSspbu9Fz87+8fv06OnfujD59+iAoKAjLli1DcHAwfHx8UKVKFQBAfHw8mjZtihcvXuDbb7+FhYUFFi9eDDMzs7fucwDYs2cPbty4gZCQEJQqVQoXLlzA4sWLceHCBRw9ehQKhQIDBgyAi4sLJk+ejCFDhqBOnTpwdHREeno6vvjiC1haWko/nBwdHQEAT548QePGjXH37l0MGDAAZcqUwZEjRzBq1CjExcVpjf0IDw/Hs2fP0L9/fyiVSpQsWfKN9X7+/LnGD0wAMDc3h7m5ea7Knj17Njp06IBevXohMzMTa9euRZcuXfDHH3+gbdu2AIDffvtN+p70798fwLtffjF0m3JzcwPw8ljYoEEDFCuW8ykoL+VcuHABDRo0gIuLi9TO1q9fj48++gibNm2Sjs1vOw9VrlwZ33//vdYlrfr16+ssN7ff2UOHDmHz5s34/PPPUbx4cfz888/o1KkTYmNjtYLSVz19+hRNmjTB9evXMXjwYHh4eGDDhg0IDg7G48ePMXToUI38q1evRmpqKgYMGACFQoGpU6fi448/xo0bN/Lcq5eQkID69evjyZMnGDJkCGxtbbFixQp06NABGzdulPa1vudzfdq4oezbtw+tW7eGj48PQkNDYWRkJP34/Oeff1C3bl0AwGeffYaNGzdi8ODB8PLywoMHD3Do0CFcunQJtWrVwujRo5GcnIw7d+5I/2NLS0v9KyIM7OTJkwKA2LNnjxBCiKysLFG6dGkxdOhQjXzjxo0TAMTmzZu11pGVlSWEEGLZsmUCgJgxY0aOefbv3y8AiP3792t8HhMTIwCI8PBwKS0oKEgAEN9++63W+p48eaKVFhYWJhQKhbh165aU1qhRI1G8eHGNtFfrI4QQo0aNEkqlUjx+/FhKS0xMFMWKFROhoaFa5bzKxsZGeHt7vzHPq+s0NTUVrVq1Emq1WkqfO3euACCWLVsmpTVu3FgAEKtXr5bSLl++LAAIIyMjcfToUSl99+7dWvsuNDRUABAdOnTQqMPnn38uAIgzZ85Iabr2pb+/vyhbtqxGmpubmwAgdu3apZXfzc1NBAUFSe+9vb1F27Zt37A3hKhRo4ZwcHAQDx48kNLOnDkjjIyMRGBgoNa29O7dW2P5jh07Cltb2zeWIcT/9uX06dOltIyMDKn8zMxMIYQQv/32mzAyMhL//POPxvILFy4UAMThw4c11lmlShWNfFu3bhUAxA8//KCR3rlzZ6FQKMT169eltOz/44ULFzTy9unTRzg5OYmkpCSN9O7duwtra2ud/6tX5fZ/efDgQSktMTFRKJVK8eWXX0ppw4YNEwDEsWPHNPJZW1sLACImJibX9VmzZo1W2dnHhQ0bNmjkrVKlimjcuLHWOiZOnCgsLCzE1atXNdK//fZbYWxsLGJjY4UQ/zuuWFlZicTExDfWNVv2vnn9lX0s0LdsXdufmZkpqlatKpo1a6aRbmFhofH9yRYUFCTc3Ny00rO/E6/KjzaVlZUlfX8cHR1Fjx49xLx587SOp7kpR9exvnnz5qJatWri2bNnGmXXr19feHp6Smn6nIdOnDihtf5sr+/P3H5nTU1NNdLOnDkjAIg5c+ZolfWqWbNmCQBi5cqVUlpmZqbw8/MTlpaWIiUlRWPf2NraiocPH0p5f//9dwFAbN++/Y3l5PQ9elX2d/rV41xqaqrw8PAQ7u7u0rlJn/O5EPq38dfPEW/z+v8xKytLeHp6Cn9/f63yPTw8RMuWLaU0a2trMWjQoDeuv23btjq/W/ow+KXJVatWwdHREU2bNgXwsvu1W7duWLt2LdRqtZRv06ZN8Pb21uo1yl4mO4+dnR2++OKLHPO8i4EDB2qlvfqLPD09HUlJSahfvz6EEPj3338BAPfv38fBgwfRu3dvlClTJsf6BAYGIiMjQ6M7dd26dXjx4sVbr02npKRo9ULlZO/evcjMzMSwYcM0Brb369cPVlZWWl3DlpaW6N69u/S+YsWKKFGiBCpXrqxxd2v23zdu3NAq8/VekOz/zc6dO6W0V/dlcnIykpKS0LhxY9y4cQPJyckay3t4eMDf3/+t21qiRAlcuHAB165d0/l5XFwcoqKiEBwcrNE7Ub16dbRs2VKjftk+++wzjfcNGzbEgwcPkJKS8tb6FCtWDAMGDJDem5qaYsCAAUhMTMSpU6cAvOxlrVy5MipVqoSkpCTp1axZMwAvL2O/yc6dO2FsbIwhQ4ZopH/55ZcQQuDPP//USG/cuDG8vLyk90IIbNq0Ce3bt4cQQqMO/v7+SE5OlrrWc5Kb/6WXl5fGIFh7e3tUrFhRox3t3LkT9erVk35pZufLHiLwNq/W59mzZ0hKSkK9evUA4K3b8iYbNmxAw4YNYWNjo7GfWrRoAbVajYMHD2rk79SpE+zt7fVev6+vL/bs2aPxCgwMzHXZr27/o0ePkJycjIYNG+Zp29/E0G1KoVBg9+7d+OGHH2BjY4M1a9Zg0KBBcHNzQ7du3aQhFXkp5+HDh9i3bx+6du2K1NRUabkHDx7A398f165dw927dwHodx7Kjdx+Z1u0aKHRW1m9enVYWVnpPPa+Xk6pUqXQo0cPKc3ExARDhgxBWloa/v77b4383bp1g42NjfQ++3v6tnL0sXPnTtStWxcffPCBlGZpaYn+/fvj5s2b0mV7fc/n76uNR0VF4dq1a+jZsycePHggtZP09HQ0b94cBw8elIYGlChRAseOHcO9e/cMWodsBr00qVarsXbtWjRt2hQxMTFSuq+vL6ZPn46IiAi0atUKwMuxAp06dXrj+qKjo1GxYsU3dl/nVrFixVC6dGmt9NjYWIwbNw7btm3Do0ePND7LPuFkN9q3zSFVqVIl1KlTB6tWrUKfPn0AvAxQ69Wr99a7R62srJCamqrXtty6dQsAtC7nmZqaomzZstLn2UqXLq11cLG2toarq6tWGgCt/QAAnp6eGu/LlSsHIyMjjfmfDh8+jNDQUERGRmqNuUpOTpbWD7wMxPTx/fff48MPP0SFChVQtWpVBAQE4NNPP0X16tUB5LwvgJeX+Xbv3q11Y8brwXT2gerRo0ewsrJ6Y32cnZ21bvKoUKECgJfjMurVq4dr167h0qVLOZ6wsweU5+TWrVtwdnbWCswrV64sff6q1/fl/fv38fjxYyxevBiLFy9+pzrk5n/5+v4EXu7TV9vRrVu3dE5pk9Ml6dc9fPgQEyZMwNq1a7Xq/npgmBvXrl3D2bNn9f5f6dtus9nZ2WncwfWuZf/xxx/44YcfEBUVJY0zBPL2w/RN8qNNKZVKjB49GqNHj0ZcXBz+/vtvzJ49G+vXr4eJiQlWrlyZp3KuX78OIQTGjh2LsWPH5risi4uLXueh3Mjtd1af70xO5Xh6emrdWa5vOa8e6/Iqp+/0q3WpWrWq3ufz99XGs3/UBwUF5ZgnOTkZNjY2mDp1KoKCguDq6gofHx+0adMGgYGBKFu2rEHqYtBAbN++fYiLi8PatWuxdu1arc9XrVolBWKGktM/59Xet1cplUqtxqtWq9GyZUs8fPgQ33zzDSpVqgQLCwvcvXsXwcHB73R7cmBgIIYOHYo7d+4gIyMDR48exdy5c9+6XKVKlRAVFYXMzEyD39pubGycq3Tx2uBSXV7f/9HR0WjevDkqVaqEGTNmwNXVFaampti5cydmzpyptS/1HRvUqFEjREdH4/fff8dff/2FX375BTNnzsTChQs1pifIjbxstz6ysrJQrVo1zJgxQ+fnrwfAefX6vsze15988kmOB5vsQFaX3P4v83t/Ai/H9R05cgRff/01atSoAUtLS2RlZSEgICBP0whkZWWhZcuWGDlypM7Ps4PsbPq2W0OW/c8//6BDhw5o1KgR5s+fDycnJ5iYmCA8PByrV6/Wq6zcHi8N3aZe5+TkhO7du6NTp06oUqUK1q9fj+XLl+epnOxlv/rqqxx72wvKdErv4zvzPsvJK0O0cX1lt5OffvpJa1qSbNnjvLp27YqGDRtiy5Yt+Ouvv/DTTz9hypQp2Lx5M1q3bp3nuhg0EFu1ahUcHBwwb948rc82b96MLVu2YOHChTAzM0O5cuVw/vz5N66vXLlyOHbsGJ4/f57jgMLsyP71uwRf/0XwJufOncPVq1exYsUK6XIBAK078rKj37fVGwC6d++OESNGYM2aNXj69ClMTEzQrVu3ty7Xvn17REZGYtOmTRrdzrpkD3y9cuWKRmSemZmJmJiYHH+B58W1a9c0fiVfv34dWVlZ0kD47du3IyMjA9u2bdP4Ffa2y3D6KFmyJEJCQhASEoK0tDQ0atQI48ePR9++fTX2xesuX74MOzs7g05Tcu/ePa0etqtXrwKAtC/KlSuHM2fOoHnz5u/0a87NzQ179+5Famqqxi/s7Ekvs7c5J/b29ihevDjUavU7tYX8+F+6ubnpvLys6//2ukePHiEiIgITJkzAuHHjpPScLlfrktP/oVy5ckhLS8uX78zb6Fv2pk2boFKpsHv3bo3pXcLDw7Xy5rSdNjY2OqeI0Pd4mdc2lRMTExNUr14d165dQ1JSUp7KyT4WmpiYvHVZfc5Dufnu5vU7m5tyzp49i6ysLI2OBUOXo29dcjruvloXfc7nuWnjeZV9SdjKykqvNubk5ITPP/8cn3/+ORITE1GrVi1MmjRJCsTy0mNnsDFiT58+xebNm9GuXTt07txZ6zV48GCkpqZKtw936tQJZ86c0TnNQ3aU3qlTJyQlJensScrO4+bmBmNjY60xHPPnz9e77tm/Fl79dSCEwOzZszXy2dvbo1GjRli2bBliY2N11iebnZ0dWrdujZUrV2LVqlUICAiAnZ3dW+vy2WefwcnJCV9++aV0Yn9VYmIifvjhBwAvxxeYmpri559/1ih/6dKlSE5ONvgdJgC0guw5c+YAgNQYde3L5OTkPH+RXp2eAHj5S6V8+fJS17WTkxNq1KiBFStWaJxozp8/j7/++gtt2rTJU/mve/HihTTVBvAy+F20aBHs7e2lCXi7du2Ku3fv6pz37enTp0hPT39jGW3atIFardZq/zNnzoRCoXjrLzFjY2N06tQJmzZt0nmyuX///luXBwz7v2zTpg2OHj2K48ePa9Qjpylu3lYfALmazdrCwkJnINK1a1dERkZi9+7dWp89fvwYL1680LuM3NK3bGNjYygUCo3eq5s3b+qcQT+n7SxXrhySk5Nx9uxZKS0uLk7ncViXvLapa9euaR07gZfbGRkZCRsbG9jb2+epHAcHBzRp0gSLFi1CXFzcG5fV5zyU/WNLnznO8vqd1VebNm0QHx+PdevWSWkvXrzAnDlzYGlpKU3f8j60adMGx48f15iWKT09HYsXL4a7u7s0xlCf83lu2nhe+fj4oFy5cpg2bRrS0tK0Ps9uJ2q1WmvYg4ODA5ydnTUunVpYWLzz8AiD9Yht27YNqamp6NChg87P69WrB3t7e6xatQrdunXD119/jY0bN6JLly7o3bs3fHx88PDhQ2zbtg0LFy6Et7c3AgMD8euvv2LEiBE4fvw4GjZsiPT0dOzduxeff/45PvzwQ1hbW6NLly6YM2cOFAoFypUrhz/++OOt4xReValSJZQrVw5fffUV7t69CysrK2zatEnn9fOff/4ZH3zwAWrVqoX+/fvDw8MDN2/exI4dO7QegREYGChNQzFx4kS96mJjY4MtW7agTZs2qFGjhsbM+qdPn8aaNWuk6S/s7e0xatQoTJgwAQEBAejQoQOuXLmC+fPno06dOvkyaV1MTAw6dOiAgIAAREZGYuXKlejZsye8vb0BAK1atYKpqSnat2+PAQMGIC0tDUuWLIGDg4POg6K+vLy80KRJE/j4+KBkyZI4efKkdDtxtp9++gmtW7eGn58f+vTpI01fYW1tjfHjx+d10zU4OztjypQpuHnzJipUqIB169YhKioKixcvln7tffrpp1i/fj0+++wz7N+/Hw0aNIBarcbly5exfv16af60nLRv3x5NmzbF6NGjcfPmTXh7e+Ovv/7C77//jmHDhuk1JcGPP/6I/fv3w9fXF/369YOXlxcePnyI06dPY+/evXj48GGOy+bH/3LkyJH47bffEBAQgKFDh0rTV2T/wn8TKysrNGrUCFOnTsXz58/h4uKCv/76S2M86tv4+PhgwYIF+OGHH1C+fHk4ODigWbNm+Prrr7Ft2za0a9dOmnIjPT0d586dw8aNG3Hz5k29fki9C33Lbtu2LWbMmIGAgAD07NkTiYmJmDdvHsqXL6+173x8fLB3717MmDEDzs7O8PDwgK+vL7p3745vvvkGHTt2xJAhQ/DkyRMsWLAAFSpU0HswdF7a1JkzZ9CzZ0+0bt0aDRs2RMmSJXH37l2sWLEC9+7dw6xZs6SAOy/lzJs3Dx988AGqVauGfv36oWzZskhISEBkZCTu3LmDM2fOSPv+beehcuXKoUSJEli4cCGKFy8OCwsL+Pr66hwnaIjvrD769++PRYsWITg4GKdOnYK7uzs2btyIw4cPY9asWXrf8KWvTZs26Xz8VFBQEL799lusWbMGrVu3xpAhQ1CyZEmsWLECMTEx2LRpk9Rjp8/5PDdtPK+MjIzwyy+/oHXr1qhSpQpCQkLg4uKCu3fvYv/+/bCyssL27duRmpqK0qVLo3PnzvD29oalpSX27t2LEydOYPr06dL6fHx8sG7dOowYMQJ16tSBpaUl2rdvr19l3uleSx3at28vVCqVSE9PzzFPcHCwMDExkW5HfvDggRg8eLBwcXERpqamonTp0iIoKEjjduUnT56I0aNHCw8PD2FiYiJKlSolOnfuLKKjo6U89+/fF506dRLm5ubCxsZGDBgwQJw/f17n9BUWFhY663bx4kXRokULYWlpKezs7ES/fv2kW4lfv235/PnzomPHjqJEiRJCpVKJihUrirFjx2qtMyMjQ9jY2Ahra2vx9OlTfXaj5N69e2L48OGiQoUKQqVSCXNzc+Hj4yMmTZokkpOTNfLOnTtXVKpUSZiYmAhHR0cxcOBA8ejRI408uqZHEOLlLcC6poUAoHG7bvbt7RcvXhSdO3cWxYsXFzY2NmLw4MFa27Zt2zZRvXp1oVKphLu7u5gyZYp06/Kr0xPkVHb2Z6/emvzDDz+IunXrihIlSggzMzNRqVIlMWnSJGmqiGx79+4VDRo0EGZmZsLKykq0b99eXLx4USNP9rbcv39fIz08PFyvKRSy9+XJkyeFn5+fUKlUws3NTcydO1crb2ZmppgyZYqoUqWKUCqVwsbGRvj4+IgJEyZo/B9z+v+kpqaK4cOHC2dnZ2FiYiI8PT3FTz/9pHG7tRDa/69XJSQkiEGDBglXV1fpO9S8eXOxePHiN26nEHn/XzZu3FhruoizZ8+Kxo0bC5VKJVxcXMTEiRPF0qVL9dr3d+7ckb571tbWokuXLuLevXsa00EIkfNt9/Hx8aJt27aiePHiAoBG3VJTU8WoUaNE+fLlhampqbCzsxP169cX06ZNk9pZ9nQAP/3001v33dv2zav0KVsIIZYuXSo8PT2FUqkUlSpVEuHh4Tqnnrh8+bJo1KiRMDMzEwA0vkt//fWXqFq1qjA1NRUVK1YUK1euzHH6CkO3qYSEBPHjjz+Kxo0bCycnJ1GsWDFhY2MjmjVrJjZu3PhO5eiavkIIIaKjo0VgYKAoVaqUMDExES4uLqJdu3Za5ehzHvr999+Fl5eXKFasmEZZuqYDyet3Vt9pGRISEkRISIiws7MTpqamolq1alr74E3t9fXvjC7Z36OcXtlTVkRHR4vOnTtL58S6deuKP/74Q2t9+pzP9W3jeZ2+Itu///4rPv74Y2FrayuUSqVwc3MTXbt2FREREUKIl+fxr7/+Wnh7e4vixYsLCwsL4e3tLebPn6+xnrS0NNGzZ09RokQJASBXU1kohChgo/UKkRcvXsDZ2Rnt27fH0qVL5a5OnmRPHHv//v186xkgIiIqaorOU5VlsHXrVty/f1/jBgAiIiKibPnyiKOi7tixYzh79iwmTpyImjVrvteBk0RERPTfwR6xfLBgwQIMHDgQDg4OOh96TERERAQAHCNGREREJBP2iBERERHJhIEYERERkUyK3GD9rKws3Lt3D8WLF8+3B+USERGRYQkhkJqaCmdnZ61nRv+XFblA7N69ewZ/2DIRERG9H7dv30bp0qXlrobBFLlALPvRD7dv34aVlZXMtSEiIiJ9pKSkwNXV1eCPcJJbkQvEsi9HWllZMRAjIiL6jylsw4oKz0VWIiIiov8YBmJEREREMpE1EDt48CDat28PZ2dnKBQKbN269a3LHDhwALVq1YJSqUT58uWxfPnyfK8nERERUX6QNRBLT0+Ht7c35s2bp1f+mJgYtG3bFk2bNkVUVBSGDRuGvn37Yvfu3flcUyIiooJj3rx5cHd3h0qlgq+vL44fP55j3ufPn+P7779HuXLloFKp4O3tjV27dmnkcXd3h0Kh0HoNGjRII19kZCSaNWsGCwsLWFlZoVGjRnj69Gm+bGNRIetg/datW6N169Z651+4cCE8PDwwffp0AEDlypVx6NAhzJw5E/7+/vlVTSIiogJj3bp1GDFiBBYuXAhfX1/MmjUL/v7+uHLlChwcHLTyjxkzBitXrsSSJUtQqVIl7N69Gx07dsSRI0dQs2ZNAMCJEyegVqulZc6fP4+WLVuiS5cuUlpkZCQCAgIwatQozJkzB8WKFcOZM2cK1ZxeshAFBACxZcuWN+Zp2LChGDp0qEbasmXLhJWVVY7LPHv2TCQnJ0uv27dvCwAiOTnZALU2jLlz5wo3NzehVCpF3bp1xbFjx3LMm5mZKSZMmCDKli0rlEqlqF69uvjzzz818ri5uQkAWq/PP/88vzeFiPLB+z5GPHjwQAwePFhUqFBBqFQq4erqKr744gvx+PHjfN1O0k/dunXFoEGDpPdqtVo4OzuLsLAwnfmdnJzE3LlzNdI+/vhj0atXrxzLGDp0qChXrpzIysqS0nx9fcWYMWPyWPt3l5ycXODO34bwnwpj4+Pj4ejoqJHm6OiIlJSUHLtGw8LCYG1tLb0K2mSu2b9sQkNDcfr0aXh7e8Pf3x+JiYk6848ZMwaLFi3CnDlzcPHiRXz22Wfo2LEj/v33XynPiRMnEBcXJ7327NkDABq/bKjgMvQlBwC4e/cuPvnkE9ja2sLMzAzVqlXDyZMnpc/T0tIwePBglC5dGmZmZvDy8sLChQvzZfsod+Q4Rty7dw/37t3DtGnTcP78eSxfvhy7du1Cnz598n+D6Y0yMzNx6tQptGjRQkozMjJCixYtEBkZqXOZjIwMqFQqjTQzMzMcOnQoxzJWrlyJ3r17S1NFJCYm4tixY3BwcED9+vXh6OiIxo0b57gOygW5I8Fs0KNHzNPTU0yePFkjbceOHQKAePLkic5lCnqPmFy/bKhgWrt2rTA1NRXLli0TFy5cEP369RMlSpQQCQkJOvOPHDlSODs7ix07dojo6Ggxf/58oVKpxOnTp6U8Dx8+FG5ubiI4OFgcO3ZM3LhxQ+zevVtcv35dytOvXz9Rrlw5sX//fhETEyMWLVokjI2Nxe+//57v20xvVlCOEevXrxempqbi+fPnudwCMqS7d+8KAOLIkSMa6V9//bWoW7euzmV69OghvLy8xNWrV4VarRZ//fWXMDMzE6ampjrzr1u3ThgbG4u7d+9KaZGRkQKAKFmypFi2bJk4ffq0GDZsmDA1NRVXr1413Aa+AXvECoBSpUohISFBIy0hIQFWVlYwMzPTuYxSqZQmby1ok7jK9cuGCq4ZM2agX79+CAkJkXqlzM3NsWzZMp35f/vtN3z33Xdo06YNypYti4EDB6JNmzbSOEoAmDJlClxdXREeHo66devCw8MDrVq1Qrly5aQ8R44cQVBQEJo0aQJ3d3f0798f3t7eb+yNo/xXkI4RycnJsLKyQrFiRW4e8P+82bNnw9PTE5UqVYKpqSkGDx6MkJCQHMd2LV26FK1bt4azs7OUlpWVBQAYMGAAQkJCULNmTcycORMVK1bM8fhE+vlPBWJ+fn6IiIjQSNuzZw/8/PxkqlHeJCUlQa1W67zcGh8fr3MZf39/zJgxA9euXUNWVhb27NmDzZs3Iy4uTmf+rVu34vHjxwgODjZ09cnA8uuku23bNtSuXRtdunSBg4MDatasiSVLlmgsU79+fWzbtg13796FEAL79+/H1atX0apVKwNuIeVWQTlGJCUlYeLEiejfv/87bwsZhp2dHYyNjXV2SpQqVUrnMvb29ti6dSvS09Nx69YtXL58GZaWlihbtqxW3lu3bmHv3r3o27evRrqTkxMAwMvLSyO9cuXKiI2NzcsmFXmyBmJpaWmIiopCVFQUgJfTU0RFRUn/1FGjRiEwMFDK/9lnn+HGjRsYOXIkLl++jPnz52P9+vUYPny4HNWXhSF+2VDBlF8n3Rs3bmDBggXw9PTE7t27MXDgQAwZMgQrVqyQ8syZMwdeXl4oXbo0TE1NERAQgHnz5qFRo0b5s7GUbwx9jEhJSUHbtm3h5eWF8ePH52PNSR+mpqbw8fHR6JTIyspCRETEWzslVCoVXFxc8OLFC2zatAkffvihVp7w8HA4ODigbdu2Gunu7u5wdnbGlStXNNKvXr0KNze3PGwRyTpGbP/+/Trv3AkKChJCCBEUFCQaN26stUyNGjWEqampKFu2rAgPD89VmQXpGnNGRoYwNjbWGhsXGBgoOnTo8MZlnz59Ku7cuSOysrLEyJEjhZeXl1aemzdvCiMjI7F161ZDVpvyybuM/UhMTBQffvihMDIyEsbGxqJChQri888/FyqVSspjYmIi/Pz8NJb74osvRL169aT3P/30k6hQoYLYtm2bOHPmjJgzZ46wtLQUe/bsMeAWUm7JfYxISUkRfn5+onnz5uLp06fvvB1kWGvXrhVKpVIsX75cXLx4UfTv31+UKFFCxMfHCyGE+PTTT8W3334r5T969KjYtGmTiI6OFgcPHhTNmjUTHh4e4tGjRxrrVavVokyZMuKbb77RWe7MmTOFlZWV2LBhg7h27ZoYM2aMUKlUGuNN81NBOn8bUoEZrP++FLR/ZN26dcXgwYOl92q1Wri4uOQ4EPd1mZmZoly5cmLUqFFan4WGhopSpUpxcO1/RH6ddMuUKSP69OmjkX/+/PnC2dlZCCHEkydPhImJifjjjz808vTp00f4+/vnYYvIEOQ6RiQnJ4t69eqJxo0bi/T09HffAMoXc+bMEWXKlBGmpqaibt264ujRo9JnjRs3ljo0hBDiwIEDonLlykKpVApbW1vx6aefagzEz7Z7924BQFy5ciXHcsPCwkTp0qWFubm58PPzE//8849Bt+tNCtr521AYiMlMrl82VDDlx0m3R48e4oMPPtDIN2zYMKmXLPs7sXPnTo08/fv3Fy1btnzXTSEDkeMYkZycLHx9fUW1atXE9evXRVxcnPR68eJFvm4vUU4K2vnbUBiIFQBy/bKhgic/TrrHjx8XxYoVE5MmTRLXrl0Tq1atEubm5mLlypVSnsaNG4sqVaqI/fv3ixs3bojw8HChUqnE/Pnz39u2U87e9zEip2EjAERMTEx+bGLRBhS+Vz4oiOdvQ2AgRlTA5MdJd/v27aJq1apCqVSKSpUqicWLF2t8HhcXJ4KDg4Wzs7NQqVSiYsWKYvr06Zx7juh9kDtoYiAmK4UQQryvGwMKgpSUFFhbW0tz4hhaYZyqq2i1EKL8xWMEaWGj0Et+n7/lwpn5iPITD7BERPQG/6kJXYmIiIgKEwZiRERERDJhIEZEREQkEwZiRERERDJhIEZEREQkEwZiRERERDJhIEZEREQkEwZiRERERDJhIEZEREQkEwZiRERERDJhIEZEREQkEwZiRERERDJhIEZEREQkEwZiRERERDJhIEZEREQkEwZiRERERDJhIEZEREQkEwZiRERERDJhIEZEREQkEwZiRERERDJhIEZEREQkEwZiRERERDJhIEZEREQkEwZiRERERDJhIEZEREQkEwZiRERERDJhIEZEREQkEwZiRERERDJhIEZEREQkEwZiRERERDJhIEZEREQkEwZiRERERDJhIEZEREQkEwZiRERERDJhIEZEREQkEwZiRERERDJhIEZEREQkEwZiRERERDJhIEZEREQkEwZiRERERDJhIEZEREQkEwZiRERERDJhIEZEREQkEwZiRERERDJhIEZEREQkEwZiRERERDJhIEZEREQkEwZiRERERDJhIEZEREQkEwZiRERERDJhIEZEREQkEwZiRERERDJhIEZEREQkEwZiRERERDJhIEZEREQkEwZiRERERDKRPRCbN28e3N3doVKp4Ovri+PHj78x/6xZs1CxYkWYmZnB1dUVw4cPx7Nnz95TbYmIiIgMR9ZAbN26dRgxYgRCQ0Nx+vRpeHt7w9/fH4mJiTrzr169Gt9++y1CQ0Nx6dIlLF26FOvWrcN33333nmtORERElHeyBmIzZsxAv379EBISAi8vLyxcuBDm5uZYtmyZzvxHjhxBgwYN0LNnT7i7u6NVq1bo0aPHW3vRiIiIiAoi2QKxzMxMnDp1Ci1atPhfZYyM0KJFC0RGRupcpn79+jh16pQUeN24cQM7d+5EmzZtciwnIyMDKSkpGi8iIiKigqCYXAUnJSVBrVbD0dFRI93R0RGXL1/WuUzPnj2RlJSEDz74AEIIvHjxAp999tkbL02GhYVhwoQJBq07ERERkSHIPlg/Nw4cOIDJkydj/vz5OH36NDZv3owdO3Zg4sSJOS4zatQoJCcnS6/bt2+/xxoTERER5Uy2HjE7OzsYGxsjISFBIz0hIQGlSpXSuczYsWPx6aefom/fvgCAatWqIT09Hf3798fo0aNhZKQdVyqVSiiVSsNvABEREVEeydYjZmpqCh8fH0REREhpWVlZiIiIgJ+fn85lnjx5ohVsGRsbAwCEEPlXWSIiIqJ8IFuPGACMGDECQUFBqF27NurWrYtZs2YhPT0dISEhAIDAwEC4uLggLCwMANC+fXvMmDEDNWvWhK+vL65fv46xY8eiffv2UkBGRERE9F8hayDWrVs33L9/H+PGjUN8fDxq1KiBXbt2SQP4Y2NjNXrAxowZA4VCgTFjxuDu3buwt7dH+/btMWnSJLk2gYiIiOidKUQRu6aXkpICa2trJCcnw8rKyuDrVygMvkrZFa0WYmBsEPQaNgnSwkahl/w+f8vlP3XXJBEREVFhwkCMiIiISCYMxIiIiIhkwkCMiIiISCYMxIiIiIhkwkCMiIiISCYMxIiIiIhkwkCMiIiISCYMxIiIiIhkwkCMiIiISCYMxIiIiIhkwkCMiIiISCYMxIiIiIhkwkCMiIiISCYMxIiIiIhkwkCMiIiISCYMxIiIiIhkwkCMiIiISCYMxIiIiIhkwkCMiIiISCYMxIiIiIhkwkCMiIiISCYMxIiIiIhkwkCMiIiISCYMxIiIiIhkwkCMiIiISCYMxIiIiIhkwkCMiIiISCYMxIiIiIhkwkCMiIiISCYMxIiIiIhkwkCMiIiISCYMxIiIiIhkwkCMiIiISCYMxIiIiIhkwkCMiIiISCYMxIiIiIhkwkCMiIiISCYMxIiIiIhkwkCMiIiISCYMxIiIiIhkwkCMiIiISCYMxIiIiIhkwkCMiIiISCYMxIiIiIhkwkCMiIiISCYMxIiIiIhkwkCMiIiISCYMxIiIiIhkwkCMiIiISCYMxIiIiIhkwkCMiIiISCYMxIiIiIhkwkCMiIiISCYMxIiIiIhkwkCMiIiISCYMxIiIiIhkwkCMiIiISCYMxIiIiIhkwkCMiIiISCYMxIiIiIhkInsgNm/ePLi7u0OlUsHX1xfHjx9/Y/7Hjx9j0KBBcHJyglKpRIUKFbBz5873VFsiIiIiwykmZ+Hr1q3DiBEjsHDhQvj6+mLWrFnw9/fHlStX4ODgoJU/MzMTLVu2hIODAzZu3AgXFxfcunULJUqUeP+VJyIiIsojhRBCyFW4r68v6tSpg7lz5wIAsrKy4Orqii+++ALffvutVv6FCxfip59+wuXLl2FiYvJOZaakpMDa2hrJycmwsrLKU/11USgMvkrZyddCCgE2CHoNmwRpYaPQS36fv+Ui26XJzMxMnDp1Ci1atPhfZYyM0KJFC0RGRupcZtu2bfDz88OgQYPg6OiIqlWrYvLkyVCr1TmWk5GRgZSUFI0XERERUUEgWyCWlJQEtVoNR0dHjXRHR0fEx8frXObGjRvYuHEj1Go1du7cibFjx2L69On44YcfciwnLCwM1tbW0svV1dWg20FERET0rmQfrJ8bWVlZcHBwwOLFi+Hj44Nu3bph9OjRWLhwYY7LjBo1CsnJydLr9u3b77HGRERERDmTbbC+nZ0djI2NkZCQoJGekJCAUqVK6VzGyckJJiYmMDY2ltIqV66M+Ph4ZGZmwtTUVGsZpVIJpVJp2MoTERERGUCue8Tc3d3x/fffIzY2Nk8Fm5qawsfHBxEREVJaVlYWIiIi4Ofnp3OZBg0a4Pr168jKypLSrl69CicnJ51BGBEREVFBlutAbNiwYdi8eTPKli2Lli1bYu3atcjIyHinwkeMGIElS5ZgxYoVuHTpEgYOHIj09HSEhIQAAAIDAzFq1Cgp/8CBA/Hw4UMMHToUV69exY4dOzB58mQMGjTonconIiIiktM7BWJRUVE4fvw4KleujC+++AJOTk4YPHgwTp8+nat1devWDdOmTcO4ceNQo0YNREVFYdeuXdIA/tjYWMTFxUn5XV1dsXv3bpw4cQLVq1fHkCFDMHToUJ1TXRAREREVdHmeR+z58+eYP38+vvnmGzx//hzVqlXDkCFDEBISAkUBnBuF84jlHucIygM2CHoNmwRpYaPQS2GdR+ydB+s/f/4cW7ZsQXh4OPbs2YN69eqhT58+uHPnDr777jvs3bsXq1evNmRdiYiIiAqVXAdip0+fRnh4ONasWQMjIyMEBgZi5syZqFSpkpSnY8eOqFOnjkErSkRERFTY5DoQq1OnDlq2bIkFCxbgo48+0vmoIQ8PD3Tv3t0gFSQiIiIqrHIdiN24cQNubm5vzGNhYYHw8PB3rhQRERFRUZDruyYTExNx7NgxrfRjx47h5MmTBqkUERERUVGQ60Bs0KBBOh8TdPfuXc7nRURERJQLuQ7ELl68iFq1amml16xZExcvXjRIpYiIiIiKglwHYkqlUuv5kAAQFxeHYsVke3QlERER0X9OrgOxVq1aYdSoUUhOTpbSHj9+jO+++w4tW7Y0aOWIiIiICrNcd2FNmzYNjRo1gpubG2rWrAkAiIqKgqOjI3777TeDV5CIiIiosMp1IObi4oKzZ89i1apVOHPmDMzMzBASEoIePXronFOMiIiIiHR7p0FdFhYW6N+/v6HrQkRERFSkvPPo+osXLyI2NhaZmZka6R06dMhzpYiIiIiKgneaWb9jx444d+4cFAoFxP8/YV3x/0+PV6vVhq0hERERUSGV67smhw4dCg8PDyQmJsLc3BwXLlzAwYMHUbt2bRw4cCAfqkhERERUOOW6RywyMhL79u2DnZ0djIyMYGRkhA8++ABhYWEYMmQI/v333/yoJxEREVGhk+seMbVajeLFiwMA7OzscO/ePQCAm5sbrly5YtjaERERERViue4Rq1q1Ks6cOQMPDw/4+vpi6tSpMDU1xeLFi1G2bNn8qCMRERFRoZTrQGzMmDFIT08HAHz//fdo164dGjZsCFtbW6xbt87gFSQiIiIqrBQi+7bHPHj48CFsbGykOycLspSUFFhbWyM5ORlWVlYGX/9/YBfkWt5bSBHGBkGvYZMgLWwUesnv87dccjVG7Pnz5yhWrBjOnz+vkV6yZMn/RBBGREREVJDkKhAzMTFBmTJlOFcYERERkQHk+q7J0aNH47vvvsPDhw/zoz5ERERERUauB+vPnTsX169fh7OzM9zc3GBhYaHx+enTpw1WOSIiIqLCLNeB2EcffZQP1SAiIiIqegxy1+R/Ce+azL2i1UIMjA2CXsMmQVrYKPTCuyaJiIiIyKByfWnSyMjojVNV8I5KIiIiIv3kOhDbsmWLxvvnz5/j33//xYoVKzBhwgSDVYyIiIiosDPYGLHVq1dj3bp1+P333w2xunzDMWK5x/EfecAGQa9hkyAtbBR64Rixt6hXrx4iIiIMtToiIiKiQs8ggdjTp0/x888/w8XFxRCrIyIiIioScj1G7PWHewshkJqaCnNzc6xcudKglSMiIiIqzHIdiM2cOVMjEDMyMoK9vT18fX1hY2Nj0MoRERERFWa5DsSCg4PzoRpERERERU+ux4iFh4djw4YNWukbNmzAihUrDFIpIiIioqIg14FYWFgY7OzstNIdHBwwefJkg1SKiIiIqCjIdSAWGxsLDw8PrXQ3NzfExsYapFJERERERUGuAzEHBwecPXtWK/3MmTOwtbU1SKWIiIiIioJcB2I9evTAkCFDsH//fqjVaqjVauzbtw9Dhw5F9+7d86OORERERIVSru+anDhxIm7evInmzZujWLGXi2dlZSEwMJBjxIiIiIhy4Z2fNXnt2jVERUXBzMwM1apVg5ubm6Hrli/4rMnc43Pk8oANgl7DJkFa2Cj0UlifNZnrHrFsnp6e8PT0NGRdiIiIiIqUXI8R69SpE6ZMmaKVPnXqVHTp0sUglSIiIiIqCnIdiB08eBBt2rTRSm/dujUOHjxokEoRERERFQW5DsTS0tJgamqqlW5iYoKUlBSDVIqIiIioKMh1IFatWjWsW7dOK33t2rXw8vIySKWIiIiIioJcD9YfO3YsPv74Y0RHR6NZs2YAgIiICKxevRobN240eAWJiIiICqtcB2Lt27fH1q1bMXnyZGzcuBFmZmbw9vbGvn37ULJkyfyoIxEREVGh9M7ziGVLSUnBmjVrsHTpUpw6dQpqtdpQdcsXnEcs9zhHUB6wQdBr2CRICxuFXgrrPGK5HiOW7eDBgwgKCoKzszOmT5+OZs2a4ejRo4asGxEREVGhlqtLk/Hx8Vi+fDmWLl2KlJQUdO3aFRkZGdi6dSsH6hMRERHlkt49Yu3bt0fFihVx9uxZzJo1C/fu3cOcOXPys25EREREhZrePWJ//vknhgwZgoEDB/LRRkREREQGoHeP2KFDh5CamgofHx/4+vpi7ty5SEpKys+6ERERERVqegdi9erVw5IlSxAXF4cBAwZg7dq1cHZ2RlZWFvbs2YPU1NT8rCcRERFRoZOn6SuuXLmCpUuX4rfffsPjx4/RsmVLbNu2zZD1MzhOX5F7vDU9D9gg6DVsEqSFjUIvnL5Ch4oVK2Lq1Km4c+cO1qxZY6g6ERERERUJeZ7Q9b+GPWK5V7RaiIGxQdBr2CRICxuFXtgjRkREREQGxUCMiIiISCYFIhCbN28e3N3doVKp4Ovri+PHj+u13Nq1a6FQKPDRRx/lbwWJiIiI8oHsgdi6deswYsQIhIaG4vTp0/D29oa/vz8SExPfuNzNmzfx1VdfoWHDhu+ppkRERESGJXsgNmPGDPTr1w8hISHw8vLCwoULYW5ujmXLluW4jFqtRq9evTBhwgSULVv2PdaWiIiIyHBkDcQyMzNx6tQptGjRQkozMjJCixYtEBkZmeNy33//PRwcHNCnT5+3lpGRkYGUlBSNFxEREVFBIGsglpSUBLVaDUdHR410R0dHxMfH61zm0KFDWLp0KZYsWaJXGWFhYbC2tpZerq6uea43ERERkSHIfmkyN1JTU/Hpp59iyZIlsLOz02uZUaNGITk5WXrdvn07n2tJREREpJ9ichZuZ2cHY2NjJCQkaKQnJCSgVKlSWvmjo6Nx8+ZNtG/fXkrLysoCABQrVgxXrlxBuXLlNJZRKpVQKpX5UHsiIiKivJG1R8zU1BQ+Pj6IiIiQ0rKyshAREQE/Pz+t/JUqVcK5c+cQFRUlvTp06ICmTZsiKiqKlx2JiIjoP0XWHjEAGDFiBIKCglC7dm3UrVsXs2bNQnp6OkJCQgAAgYGBcHFxQVhYGFQqFapWraqxfIkSJQBAK52IiIiooJM9EOvWrRvu37+PcePGIT4+HjVq1MCuXbukAfyxsbEwMvpPDWUjIiIi0gsf+m1gfHYraWCDoNewSZAWNgq98KHfRERERGRQDMSIiIiIZMJAjIiIiEgmDMSIiIiIZMJAjIiIiEgmDMSIiIiIZMJAjIiIiEgmDMSIiIiIZMJAjIiIiEgmDMSIiIiIZMJAjIiIiEgmDMSIiIiIZMJAjIiIiEgmDMSIiIiIZMJAjIiIiEgmDMSIiIiIZMJAjIiIiEgmDMSIiIiIZMJAjIiIiEgmDMSIiIiIZMJAjIiIiEgmDMSIiIiIZMJAjIiIiEgmDMSIiIiIZMJAjIiIiEgmDMSIiIiIZMJAjIiIiEgmDMSIiIiIZMJAjIiIiEgmDMSIiIiIZMJAjIiIiEgmDMSIiIiIZMJAjIiIiEgmDMSIiIiIZMJAjIiIiEgmDMSIiIiIZMJAjIiIiEgmDMSIiIiIZMJAjIiIiEgmDMSIiIiIZMJAjIiIiEgmDMSIiIiIZMJAjIiIiEgmDMSIiIiIZMJAjIiIiEgmDMSIiIiIZMJAjIiIiEgmDMSIiIiIZMJAjIiIiEgmDMSIiIiIZMJAjIiIiEgmDMSIiIiIZMJAjIiIiEgmDMSIiIiIZMJAjIiIiEgmDMSIiIiIZMJAjIiIiEgmDMSIiIiIZMJAjIiIiEgmDMSIiIiIZMJAjIiIiEgmBSIQmzdvHtzd3aFSqeDr64vjx4/nmHfJkiVo2LAhbGxsYGNjgxYtWrwxPxEREVFBJXsgtm7dOowYMQKhoaE4ffo0vL294e/vj8TERJ35Dxw4gB49emD//v2IjIyEq6srWrVqhbt3777nmhMRERHljUIIIeSsgK+vL+rUqYO5c+cCALKysuDq6oovvvgC33777VuXV6vVsLGxwdy5cxEYGPjW/CkpKbC2tkZycjKsrKzyXP/XKRQGX6Xs5G0h/3FsEPQaNgnSwkahl/w+f8tF1h6xzMxMnDp1Ci1atJDSjIyM0KJFC0RGRuq1jidPnuD58+coWbKkzs8zMjKQkpKi8SIiIiIqCGQNxJKSkqBWq+Ho6KiR7ujoiPj4eL3W8c0338DZ2VkjmHtVWFgYrK2tpZerq2ue601ERERkCLKPEcuLH3/8EWvXrsWWLVugUql05hk1ahSSk5Ol1+3bt99zLYmIiIh0KyZn4XZ2djA2NkZCQoJGekJCAkqVKvXGZadNm4Yff/wRe/fuRfXq1XPMp1QqoVQqDVJfIiIiIkOStUfM1NQUPj4+iIiIkNKysrIQEREBPz+/HJebOnUqJk6ciF27dqF27drvo6pEREREBidrjxgAjBgxAkFBQahduzbq1q2LWbNmIT09HSEhIQCAwMBAuLi4ICwsDAAwZcoUjBs3DqtXr4a7u7s0lszS0hKWlpaybQcRERFRbskeiHXr1g3379/HuHHjEB8fjxo1amDXrl3SAP7Y2FgYGf2v427BggXIzMxE586dNdYTGhqK8ePHv8+qExEREeWJ7POIvW+cRyz3ilYLMTA2CHoNmwRpYaPQC+cRIyIiIiKDYiBGREREJBMGYkREREQyYSBGREREJBMGYkREREQyYSBGREREJBMGYkREREQyYSBGREREJBMGYkREREQyYSBGREREJBMGYkREREQykf2h30RERAWVWq3G8+fP87cQN7f8Xb8cnj17p8VMTU1hZFS0+ogYiBEREb1GCIH4+Hg8fvw4/wtbuDD/y3jfYmLeaTEjIyN4eHjA1NTUwBUquBiIERERvSY7CHNwcIC5uTkUCkX+FZaenn/rlouHR64XycrKwr179xAXF4cyZcrk7z4vQBiIERERvUKtVktBmK2trdzV+W9Sqd5pMXt7e9y7dw8vXryAiYmJgStVMBWtC7FERERvkT0mzNzcXOaaFD3ZlyTVarXMNXl/GIgRERHpUFQujRUkRXGfMxAjIiIikgkDMSIiokKiyYABGDZ9ep7WsfXAAZTv2BHGvr55Xhe9HQfrExER6Sl/rpzVzvETceJkfhT4RgPCwhDSrh2GdO+O4hwnl+8YiBEREREAIO3JEyQ+fAh/Pz8429u/83oyMzOL1FxgecFLk0RERIXIC7Uag6dOhXWTJrBr0QJjFyyAEAIAkJGZia9mzYJLmzawaNgQvsHBOHDqFADgwKlTKN64MQCg2cCBUNSpI322ad8+VOnaFcr69eHeoQOmr1ypUaZ7hw6Y+MsvCAwNhVWTJujfvz8A4NChQ2jYsCHMzMzg6uqKIUOGIL0wzpuWBwzEiIiICpEVO3agmLExji9fjtlffokZq1fjl61bAQCDp05F5LlzWDtpEs6uWYMuzZsjYMgQXIuNRf3q1XFl40YAwKYpUxD355+oX706Tl26hK6jRqF7q1Y4t2YNxvfrh7ELF2L59u0a5U5buRLenp74d+VKjB07FtHR0QgICECnTp1w9uxZrFu3DocOHcLgwYPf9y4p0HhpkoiIqBBxdXTEzBEjoFAoUNHdHeeuX8fMNWvg7+eH8D/+QOz27dJlx68+/RS7IiMRvn07Jg8aBIeSJQEAJa2tUcrODgAwY9UqNK9TB2P79gUAVHBzw8WYGPz0228Ibt9eKrdZnTr48pNPXr4pVw59+/ZFr169MGzYMACAp6cnfv75ZzRu3BgLFiyA6h0nfS1sGIgREREVIvWqVtWYj8uvenVMX7UK565fh1qtRoVOnTTyZ2RmwtbaOsf1Xbp5Ex/+/yXLbA28vTFrzRqo1WoYGxsDAGpXrqyR58yZMzh79ixWrVolpQkhkJWVhZiYGFR+LX9RxUCMiIioCEh78gTGxsY49euvUvCUzdLMLM/rt3ithystLQ0DBgzAkCFDtPKWKVMmz+UVFgzEiIiICpFj589rvD967hw8y5RBzYoVoVarkfjoERrWrKn3+iq7u+PwmTMaaYfPnEGFMmW0ArpX1apVCxcvXkT58uVztwFFDAfrExERFSKxCQkYMXMmrty8iTW7d2PO+vUY2r07Kri5oVdAAALHj8fmffsQc/cujl+4gLDwcOw4dCjH9X35ySeIOHECE3/5BVdv3cKKP/7A3PXr8VX2eLAcfPPNNzhy5AgGDx6MqKgoXLt2Db///jsH67+GPWJERESFSGCbNniakYG6wcEwNjbG0O7d0b9jRwBAeGgofli6FF/Ono27iYmwK1EC9apWRbuGDXNcX61KlbA+LAzjFi7ExKVL4WRnh+8HDNAYqK9L9erV8ffff2P06NFo2LAhhBAoV64cunXrZtDt/a9TiOzJRYqIlJQUWFtbIzk5GVZWVgZff2F8XmnRaiEGxgZBr2GTKPiePXuGmJgYeHh4vJ87+06+/9nz813tnJ8W8CZv2vf5ff6WCy9NEhEREcmEgRgRERGRTBiIEREREcmEgRgRERGRTBiIEREREcmEgRgRERGRTBiIEREREcmEgRgRERGRTBiIEREREcmEgRgREVERcvnmTdQLCYGqQQPU6NlT7uoUeXzWJBERkb7e9zOqTpww+CpDFy2ChUqFKxs3wtLMDMu3b8ewGTPweP9+g5dFb8dAjIiIqAiJvnsXbRs0gJuTk0HXq1aroVAoYGTEi225wb1FRERUiOw6cgQf9O2LEk2bwrZFC7QbPhzRd+4AABR16uDUpUv4/pdfoKhTB00GDEDI998jOS0Nijp1oKhTB+MXLwYAZGRm4qtZs+DSpg0sGjaEb3AwDpw6JZWzfPt2lGjaFNv+/hteXbtC2aABYuPjZdnm/zL2iBERERUi6c+eYUTPnqju6Ym0J08wbtEidPz6a0StWoW4P/9Ei0GDEODnh68++QTmKhXCt2/HuEWLcGXjRgCApbk5AGDw1Km4GBODtZMmwdneHlv270fAkCE4t2YNPMuUAQA8efYMU379Fb+MHg1ba2s4lCwp23b/VzEQIyIiKkQ6NWum8X7ZuHGwb9kSF2/cQNXy5VHM2BiW5uYoZWcHALC2tIRCoZDeA0BsfDzC//gDsdu3w9neHgDw1aefYldkJMK3b8fkQYMAAM9fvMD8b76Bd4UK72nrCh8GYkRERIXItdhYjFu0CMfOn0dScjKysrIAALEJCahavrxe6zh3/TrUajUqdOqkkZ6RmQlba2vpvamJCap7ehqu8kUQAzEiIqJCpP2IEXBzcsKS0aPhbG+PrKwsVO3eHZnPn+u9jrQnT2BsbIxTv/4KY2Njjc8szcykv82USije952khQwDMSIiokLiwePHuHLrFpaMHo2GNWsCAA5FRb1xGVMTE6j/v9csW82KFaFWq5H46JG0HsofvGuSiIiokLCxsoKttTUWb9mC67dvY9+JExgxc+Ybl3F3ckLakyeIOH4cSY8f48mzZ6jg5oZeAQEIHD8em/ftQ8zduzh+4QLCwsOx49Ch97Q1RQMDMSIiokLCyMgIaydNwqnLl1G1e3cMnzkTPw0Z8sZl6nt747NOndDtu+9g37Ilpv76KwAgPDQUgW3a4MvZs1Gxc2d89NVXOHHxIsqUKvU+NqXIUAghhNyVeJ9SUlJgbW2N5ORkWFlZGXz9hfFSedFqIQbGBkGvYZMo+J49e4aYmBh4eHhApVLlf4EnT+Z/Ge9b7drvtNib9n1+n7/lwh4xIiIiIpkwECMiIiKSCQMxIiIiIpkwECMiIiKSCQMxIiIiIpkwECMiItIh67VJTin/FbGJHABwZn0iIiINpqamMDIywr1792Bvbw9TU1M+xie3nj3L9SJCCNy/fx8KhQImJib5UKmCiYEYERHRK4yMjODh4YG4uDjcu3cv/wtMSsr/Mt63mJh3WkyhUKB06dJaz7cszBiIERERvcbU1BRlypTBixcvoFar87ew1q3zd/1yuHz5nRYzMTEpUkEYwECMiIhIp+xLZPl+mezWrfxdvxzexxMJCokCMVh/3rx5cHd3h0qlgq+vL44fP/7G/Bs2bEClSpWgUqlQrVo17Ny58z3VlIiIiMhwZA/E1q1bhxEjRiA0NBSnT5+Gt7c3/P39kZiYqDP/kSNH0KNHD/Tp0wf//vsvPvroI3z00Uc4f/78e645ERERUd7I/tBvX19f1KlTB3PnzgXw8nZhV1dXfPHFF/j222+18nfr1g3p6en4448/pLR69eqhRo0aWLhw4VvL40O/c68I3k1sOGwQ9Bo2CdLCRqGXwvrQb1nHiGVmZuLUqVMYNWqUlGZkZIQWLVogMjJS5zKRkZEYMWKERpq/vz+2bt2qM39GRgYyMjKk98nJyQBe/kNJP9xVpIENgl7DJkFa8qFRZJ+3C9tcY7IGYklJSVCr1XB0dNRId3R0xOUc7riIj4/XmT8+Pl5n/rCwMEyYMEEr3dXV9R1rXfRYW8tdAypQ2CDoNWwSpCUfG0VqaiqsC1GjK/R3TY4aNUqjBy0rKwsPHz6Era0tJ+grBFJSUuDq6orbt28Xqq5qIjIMHiMKDyEEUlNT4ezsLHdVDErWQMzOzg7GxsZISEjQSE9ISECpUqV0LlOqVKlc5VcqlVAqlRppJUqUePdKU4FkZWXFgywR5YjHiMKhMPWEZZP1rklTU1P4+PggIiJCSsvKykJERAT8/Px0LuPn56eRHwD27NmTY34iIiKigkr2S5MjRoxAUFAQateujbp162LWrFlIT09HSEgIACAwMBAuLi4ICwsDAAwdOhSNGzfG9OnT0bZtW6xduxYnT57E4sWL5dwMIiIiolyTPRDr1q0b7t+/j3HjxiE+Ph41atTArl27pAH5sbGxMDL6X8dd/fr1sXr1aowZMwbfffcdPD09sXXrVlStWlWuTSAZKZVKhIaGal1+JiICeIyggk/2ecSIiIiIiirZZ9YnIiIiKqoYiBERERHJhIEYERERkUwYiBERERHJhIEYvRfLly8vtBPpBgcH46OPPtI7/4EDB6BQKPD48eN8qxORPhQKRY7P6TWkJk2aYNiwYdJ7d3d3zJo1S3ofHx+Pli1bwsLCQjpOGKJuuf1uEsmBgRgBeHnAUigU0svW1hYBAQE4e/asQdbfrVs3XL16Ve/8TZo00ahP9uvFixdSnuvXryMkJASlS5eGUqmEh4cHevTogZMnT0p5spc7evSoxvozMjKkx1wdOHAgz9tHVBDdv38fAwcORJkyZaBUKlGqVCn4+/vj8OHDAIC4uDi0bt36vdfrxIkT6N+/v/R+5syZiIuLQ1RUlHScyI+6vXqcMzExgYeHB0aOHIlnz55p5HtbELhkyRJ4e3vD0tISJUqUQM2aNaW5LrPLMWQA+HogS4ULAzGSBAQEIC4uDnFxcYiIiECxYsXQrl07g6zbzMwMDg4OuVqmX79+Un2yX8WKvZz67uTJk/Dx8cHVq1exaNEiXLx4EVu2bEGlSpXw5ZdfaqzH1dUV4eHhGmlbtmyBpaVl3jaKqIDr1KkT/v33X6xYsQJXr17Ftm3b0KRJEzx48ADAy0fGyTG/lr29PczNzaX30dHR8PHxgaenp3ScyK+6ZR/nbty4gZkzZ2LRokUIDQ3Ve/lly5Zh2LBhGDJkCKKionD48GGMHDkSaWlpBq9rZmZmgV4fGYggEkIEBQWJDz/8UCPtn3/+EQBEYmKiEEKIkSNHCk9PT2FmZiY8PDzEmDFjRGZmppQ/KipKNGnSRFhaWorixYuLWrVqiRMnTgghhAgPDxfW1tYa69+2bZuoXbu2UCqVwtbWVnz00UfSZ40bNxZDhw7VWdesrCxRpUoV4ePjI9Rqtdbnjx49kv4GIMaMGSOsrKzEkydPpPSWLVuKsWPHCgBi//79UvrZs2dF06ZNhUqlEiVLlhT9+vUTqamp0ucvXrwQw4cPF9bW1qJkyZLi66+/FoGBgRr7Tq1Wi8mTJwt3d3ehUqlE9erVxYYNG6TP9+/fLwBo1JPI0B49eiQAiAMHDuSYB4DYsmWLEEKImJgYAUCsW7dOfPDBB0KlUonatWuLK1euiOPHjwsfHx9hYWEhAgICpGOCEP87dowfP17Y2dmJ4sWLiwEDBoiMjAwpz+vfZzc3NzFz5kzpbwDSKygoSKtuQggRGxsrunTpIqytrYWNjY3o0KGDiImJkT7X57up6zj38ccfi5o1a+a4X1734YcfiuDg4Bz3aWhoqMb2vHqMedsxNDQ0VHh7e4slS5YId3d3oVAoRFBQkNb6srf73LlzIiAgQFhYWAgHBwfxySefiPv372vs90GDBomhQ4cKW1tb0aRJkxzrTfJhjxjplJaWhpUrV6J8+fKwtbUFABQvXhzLly/HxYsXMXv2bCxZsgQzZ86UlunVqxdKly6NEydO4NSpU/j2229hYmKic/07duxAx44d0aZNG/z777+IiIhA3bp19apbVFQULly4gC+//FLjqQvZXh+L5uPjA3d3d2zatAnAy6c1HDx4EJ9++qlGvvT0dPj7+8PGxgYnTpzAhg0bsHfvXgwePFjKM336dCxfvhzLli3DoUOH8PDhQ2zZskVjPWFhYfj111+xcOFCXLhwAcOHD8cnn3yCv//+W6/tIzIES0tLWFpaYuvWrcjIyNB7udDQUIwZMwanT59GsWLF0LNnT4wcORKzZ8/GP//8g+vXr2PcuHEay0RERODSpUs4cOAA1qxZg82bN2PChAl6lXfixAkEBASga9euiIuLw+zZs7XyPH/+HP7+/ihevDj++ecfHD58GJaWlggICJB6efT5br7u/PnzOHLkCExNTfXcOy976o4ePYpbt27p/Pyrr75C165dNa4w1K9fH8Dbj6HAyyEXmzZtwubNmxEVFYXZs2fDz89P4wqBq6srHj9+jGbNmqFmzZo4efIkdu3ahYSEBHTt2lVjfStWrICpqSkOHz6MhQsX6r2d9B7JHQlSwRAUFCSMjY2FhYWFsLCwEACEk5OTOHXqVI7L/PTTT8LHx0d6X7x4cbF8+XKdeV/vEfPz8xO9evXKcd2NGzcWJiYmUn0sLCzEiBEjhBBCrFu3TgAQp0+ffut24f9/2c6aNUs0bdpUCCHEhAkTRMeOHaUeg+xfq4sXLxY2NjYiLS1NWn7Hjh3CyMhIxMfHCyGEcHJyElOnTpU+f/78uShdurT0K/vZs2fC3NxcHDlyRKMeffr0ET169BBCsEeM3p+NGzcKGxsboVKpRP369cWoUaPEmTNnpM+ho0fsl19+kT5fs2aNACAiIiKktLCwMFGxYkXpfVBQkChZsqRIT0+X0hYsWCAsLS2lHus39YgJ8bKXKbsnTFfdfvvtN1GxYkWRlZUlfZ6RkSHMzMzE7t27hRBv/25m1zX7OKdUKgUAYWRkJDZu3Jhj2a+7d++eqFevngAgKlSoIIKCgsS6des0eud19bzp8voxNDQ0VJiYmGj0OAqh+wrBxIkTRatWrTTSbt++LQCIK1euSMu93ttHBQ97xEjStGlTREVFISoqCsePH4e/vz9at24t/fJbt24dGjRogFKlSsHS0hJjxoxBbGystPyIESPQt29ftGjRAj/++COio6NzLCsqKgrNmzd/Y3169eol1ScqKgqjRo0CAIh3eCrXJ598gsjISNy4cQPLly9H7969tfJcunQJ3t7esLCwkNIaNGiArKwsXLlyBcnJyYiLi4Ovr6/0ebFixVC7dm3p/fXr1/HkyRO0bNlS6pGwtLTEr7/++sb9QZQfOnXqhHv37mHbtm0ICAjAgQMHUKtWLSxfvjzHZapXry79nf3M32rVqmmkJSYmaizj7e2tMebLz88PaWlpuH37tkG248yZM7h+/TqKFy8ufadKliyJZ8+eITo6Wq/vZrbs49yxY8cQFBSEkJAQdOrUSe+6ODk5ITIyEufOncPQoUPx4sULBAUFISAgAFlZWW9c9m3HUABwc3ODvb39W+tx5swZ7N+/X+M4U6lSJQDQONb4+PjovW0kD9kf+k0Fh4WFBcqXLy+9/+WXX2BtbY0lS5agbdu26NWrFyZMmAB/f39YW1tj7dq1mD59upR//Pjx6NmzJ3bs2IE///wToaGhWLt2LTp27KhVlpmZ2VvrY21trVGfbBUqVAAAXL58GTVr1tRr22xtbdGuXTv06dMHz549Q+vWrZGamqrXsrmRPWB3x44dcHFx0fiMDx0mOahUKrRs2RItW7bE2LFj0bdvX4SGhiI4OFhn/leHEygUCp1pbws4DC0tLQ0+Pj5YtWqV1mf6BC2vevU4t2zZMnh7e2Pp0qXo06dPrtZTtWpVVK1aFZ9//jk+++wzNGzYEH///TeaNm2qM39kZORbj6HZ9dNHWloa2rdvjylTpmh95uTklOv1kXzYI0Y5UigUMDIywtOnT3HkyBG4ublh9OjRqF27Njw9PXWOkahQoQKGDx+Ov/76Cx9//LHW3YrZqlevjoiIiHeqV40aNeDl5YXp06frPCHkND9X7969ceDAAQQGBsLY2Fjr88qVK+PMmTNIT0+X0g4fPgwjIyNUrFgR1tbWcHJywrFjx6TPX7x4gVOnTknvvby8oFQqERsbi/Lly2u8XF1d32l7iQzJy8tLo40bwpkzZ/D06VPp/dGjR2FpaWmwNl+rVi1cu3YNDg4OWt8ra2trvb6buhgZGeG7777DmDFjNOqfW15eXgAg7VdTU1Oo1WqNPPoeQ3XRtb5atWrhwoULcHd319onDL7+WxiIkSQjIwPx8fGIj4/HpUuX8MUXX0i/ujw9PREbG4u1a9ciOjoaP//8s8ZA2KdPn2Lw4ME4cOAAbt26hcOHD+PEiROoXLmyzrJCQ0OxZs0ahIaG4tKlSzh37pzOX3a6KBQKhIeH4+rVq2jYsCF27tyJGzdu4OzZs5g0aRI+/PBDncsFBATg/v37+P7773V+3qtXL6hUKgQFBeH8+fPYv38/vvjiC3z66afSJZqhQ4fixx9/xNatW3H58mV8/vnnGoFf8eLF8dVXX2H48OFYsWIFoqOjcfr0acyZMwcrVqzQa/uIDOHBgwdo1qwZVq5cibNnzyImJgYbNmzA1KlTc/yOvKvMzEz06dMHFy9exM6dOxEaGorBgwfrvJnmXfTq1Qt2dnb48MMP8c8//yAmJgYHDhzAkCFDcOfOHQBv/27mpEuXLjA2Nsa8efM00mNiYjSGRkRFRSE9PR0DBw7ExIkTcfjwYdy6dQtHjx5FYGAg7O3t4efnB+DlhLVnz57FlStXkJSUhOfPn7/1GPom7u7uOHbsGG7evImkpCRkZWVh0KBBePjwIXr06IETJ04gOjoau3fvRkhIiFbQRgUbL02SZNeuXVKXdvHixVGpUiVs2LABTZo0AQAMHz4cgwcPRkZGBtq2bYuxY8di/PjxAABjY2M8ePAAgYGBSEhIgJ2dHT7++OMc75xq0qQJNmzYgIkTJ+LHH3+ElZUVGjVqpHdd69ati5MnT2LSpEno168fkpKS4OTkhPr162vM2P0qhUIBOzu7HNdpbm6O3bt3Y+jQoahTpw7Mzc3RqVMnzJgxQ8rz5ZdfIi4uDkFBQTAyMkLv3r3RsWNHJCcnS3kmTpwIe3t7hIWF4caNGyhRogRq1aqF7777Tu/tI8orS0tL+Pr6YubMmYiOjsbz58/h6uqKfv36GbwtNm/eHJ6enmjUqBEyMjLQo0cP6dhgCObm5jh48CC++eYbfPzxx0hNTYWLiwuaN28OKysrAPp9N3UpVqwYBg8ejKlTp2LgwIFSb9KIESO08v7zzz9o0aIFli1bhgULFuDBgwews7ODn58fIiIipDvM+/XrhwMHDqB27dpIS0vD/v370aFDhzceQ9/kq6++QlBQELy8vPD06VPExMTA3d0dhw8fxjfffINWrVohIyMDbm5uCAgIMFgATO+HQrzLyGciIiK8nEX+8ePH7+VRSUSFEcNmIiIiIpkwECMiIiKSCS9NEhEREcmEPWJEREREMmEgRkRERCQTBmJEREREMmEgRkRERCQTBmJEREREMmEgRkRERCQTBmJEREREMmEgRkRERCST/wPVXI9hiZ1pUAAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "fig, ax1 = plt.subplots()\n",
    "# ax2 = ax1.twinx() \n",
    "ax1.bar([0, 2],  [.97, .9717], width=.4, color='blue', label='before');\n",
    "# ax1.bar([1, 3], [.1160, .1142], width=.4, color='green', label='after');\n",
    "ax1.bar([1, 3], [0.9683, 0.9758], width=.4, color='red', label='after');\n",
    "ax1.set_ylabel('Accuracy', color='black');\n",
    "# ax1.tick_params(axis='y', labelcolor='orange');\n",
    "# ax2.set_ylabel('Accuracy', color='b');\n",
    "# ax2.tick_params(axis='y', labelcolor='b');\n",
    "# ax1.set_xticks([0, 2], ['BasicFCModel', 'SimplifiedRLStarter'], rotation=5);\n",
    "ax1.set_xticks([.5, 2.5], ['BasicFCModel', 'SimplifiedRLStarter']);\n",
    "for ind, val in zip(range(4), [.97, 0.9683, .9717, 0.9758]):\n",
    "    ax = ax1 #if ind % 2 == 0 else ax2\n",
    "    h_add = .005 #if ind % 2 == 0 else .005\n",
    "    ax.text(ind-.125, val+h_add, s=str(round(val, 3)))\n",
    "plt.title('Accuracy Comparison before and after Feature Selection on Local Test');\n",
    "plt.legend(loc=4)\n",
    "plt.savefig('./extracted_features/acc_comparison.png', dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3765,)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fe_importance['SimplifiedRLStarter'][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fe_importance['BasicFCModel'][0][1463]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9992460058629061"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imp = fe_importance['BasicFCModel'][0]\n",
    "\n",
    "np.sum(imp[np.argsort(imp)[::-1][:800]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([   0,    1,    3, ..., 2346, 2348, 2349]),)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.nonzero(fe_importance['BasicFCModel'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.hist(np.asarray(fe_importance['BasicFCModel'][0])[np.nonzero(fe_importance['BasicFCModel'][0])[0]], bins=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "importance_level = np.argsort(fe_importance['BasicFCModel'][0])[::-1][:np.count_nonzero(fe_importance['BasicFCModel'][0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([9.43464973e-02, 7.12541787e-02, 6.02651122e-02, ...,\n",
       "       2.85341630e-10, 1.57271242e-10, 1.38377875e-19])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.asarray(fe_importance['BasicFCModel'][0])[importance_level]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   0,    1,    2, ..., 2346, 2348, 2349])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.nonzero(fe_importance['BasicFCModel'][0])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "fe_imp_ind = {'BasicFCModel':2200, 'SimplifiedRLStarter':3200}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "fe_importance_set = {}\n",
    "for k, vs in fe_importance.items():\n",
    "    imp = set(np.argsort(vs[0])[::-1][:fe_imp_ind[k]])\n",
    "    for v in vs:\n",
    "        valid_inds = np.argsort(v)[::-1][:fe_imp_ind[k]]\n",
    "        imp = imp.intersection(valid_inds)\n",
    "    fe_importance_set[k] = imp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BasicFCModel - 795\n",
      "SimplifiedRLStarter - 1081\n"
     ]
    }
   ],
   "source": [
    "for k, v in fe_importance_set.items():\n",
    "    print(f'{k} - {len(v)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k, v in fe_importance_set.items():\n",
    "    fe_importance_set[k] = list(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(118, 1878)\n",
      "(120, 2466)\n"
     ]
    }
   ],
   "source": [
    "X_fe_reduced = {}\n",
    "for k, v in X.items():\n",
    "    X_fe_reduced[k] = np.asarray(v)[:, fe_importance_set[k]]\n",
    "    print(X_fe_reduced[k].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k, v in fe_importance_set.items():\n",
    "    fe_importance_set[k] = [int(l) for l in list(v)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(118, 795)\n",
      "(120, 1081)\n"
     ]
    }
   ],
   "source": [
    "X_fe_reduced = {}\n",
    "for k, v in X.items():\n",
    "    X_fe_reduced[k] = np.asarray(v)[:, fe_importance_set[k]]\n",
    "    print(X_fe_reduced[k].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "json.dump(fe_importance_set, open(os.path.join(OUTPUT_FILEDIR, 'fe_ind2.json'), 'w'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BasicFCModel 0.11665780598913689 0.9641666666666667\n",
      "SimplifiedRLStarter 0.10297708796412654 0.9791666666666665\n"
     ]
    }
   ],
   "source": [
    "for k, v in X_fe_reduced.items():\n",
    "    cen, acc = bootstrap_performance(v, y[k], clf, n=50, test_size=.2)\n",
    "    print(k, np.mean(cen), np.mean(acc))\n",
    "    # fe_importance[k] = fe_rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = GradientBoostingClassifier(n_estimators=100, learning_rate=0.111, max_depth=2, min_samples_leaf=6, min_samples_split=16, subsample=.6, max_features=300)\n",
    "# param={'max_depth': range(2, 9, 2), 'min_samples_leaf': range(2, 35, 4), 'min_samples_split': range(16, 65, 4), 'max_features': range(200, 411, 20), 'subsample': np.arange(.6, .91, .02)}\n",
    "# param = {'learning_rate':np.arange(.05, .301, .05), 'n_estimators':range(100, 2001, 25)}\n",
    "param = {'learning_rate':np.arange(.001, .301, .001), 'n_estimators':range(100, 3001, 25)}\n",
    "# param = {'learning_rate':[.015, .03, .005, .01, .0025, .025, .003, .0015, .0005], 'n_estimators':[125, 63, 375, 187, 750, 75, 630, 1250, 3750]}\n",
    "gsearch = GridSearchCV(estimator=clf, param_grid=param, scoring=['neg_log_loss', 'accuracy'], n_jobs=10, cv=5, refit=False);\n",
    "gsearch.fit(X, y);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "gsearch_result = pd.DataFrame(gsearch.cv_results_).sort_values(by=['rank_test_neg_log_loss', 'rank_test_accuracy'])\n",
    "gsearch_result.to_csv(os.path.join(OUTPUT_FILEDIR, f'gsearch_result_{k}.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_learning_rate</th>\n",
       "      <th>param_n_estimators</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_neg_log_loss</th>\n",
       "      <th>split1_test_neg_log_loss</th>\n",
       "      <th>split2_test_neg_log_loss</th>\n",
       "      <th>...</th>\n",
       "      <th>std_test_neg_log_loss</th>\n",
       "      <th>rank_test_neg_log_loss</th>\n",
       "      <th>split0_test_accuracy</th>\n",
       "      <th>split1_test_accuracy</th>\n",
       "      <th>split2_test_accuracy</th>\n",
       "      <th>split3_test_accuracy</th>\n",
       "      <th>split4_test_accuracy</th>\n",
       "      <th>mean_test_accuracy</th>\n",
       "      <th>std_test_accuracy</th>\n",
       "      <th>rank_test_accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3745</th>\n",
       "      <td>0.078553</td>\n",
       "      <td>0.000611</td>\n",
       "      <td>0.001505</td>\n",
       "      <td>0.000028</td>\n",
       "      <td>0.033</td>\n",
       "      <td>125</td>\n",
       "      <td>{'learning_rate': 0.033, 'n_estimators': 125}</td>\n",
       "      <td>-0.341112</td>\n",
       "      <td>-0.549322</td>\n",
       "      <td>-0.706908</td>\n",
       "      <td>...</td>\n",
       "      <td>0.140328</td>\n",
       "      <td>1</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>0.750</td>\n",
       "      <td>0.250</td>\n",
       "      <td>0.622222</td>\n",
       "      <td>0.237398</td>\n",
       "      <td>32209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8073</th>\n",
       "      <td>0.063099</td>\n",
       "      <td>0.001126</td>\n",
       "      <td>0.001582</td>\n",
       "      <td>0.000049</td>\n",
       "      <td>0.07</td>\n",
       "      <td>100</td>\n",
       "      <td>{'learning_rate': 0.07, 'n_estimators': 100}</td>\n",
       "      <td>-0.356271</td>\n",
       "      <td>-0.657309</td>\n",
       "      <td>-0.693475</td>\n",
       "      <td>...</td>\n",
       "      <td>0.131424</td>\n",
       "      <td>2</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.750</td>\n",
       "      <td>0.250</td>\n",
       "      <td>0.644444</td>\n",
       "      <td>0.213293</td>\n",
       "      <td>27848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12870</th>\n",
       "      <td>0.071993</td>\n",
       "      <td>0.008178</td>\n",
       "      <td>0.001946</td>\n",
       "      <td>0.000462</td>\n",
       "      <td>0.111</td>\n",
       "      <td>100</td>\n",
       "      <td>{'learning_rate': 0.111, 'n_estimators': 100}</td>\n",
       "      <td>-0.228063</td>\n",
       "      <td>-0.557090</td>\n",
       "      <td>-0.718041</td>\n",
       "      <td>...</td>\n",
       "      <td>0.212997</td>\n",
       "      <td>3</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.750</td>\n",
       "      <td>0.625</td>\n",
       "      <td>0.719444</td>\n",
       "      <td>0.093953</td>\n",
       "      <td>2464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5850</th>\n",
       "      <td>0.062990</td>\n",
       "      <td>0.000678</td>\n",
       "      <td>0.001561</td>\n",
       "      <td>0.000037</td>\n",
       "      <td>0.051</td>\n",
       "      <td>100</td>\n",
       "      <td>{'learning_rate': 0.051000000000000004, 'n_est...</td>\n",
       "      <td>-0.352012</td>\n",
       "      <td>-0.573338</td>\n",
       "      <td>-0.721012</td>\n",
       "      <td>...</td>\n",
       "      <td>0.142894</td>\n",
       "      <td>4</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.625</td>\n",
       "      <td>0.375</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.172357</td>\n",
       "      <td>20042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5968</th>\n",
       "      <td>0.079375</td>\n",
       "      <td>0.001292</td>\n",
       "      <td>0.001526</td>\n",
       "      <td>0.000033</td>\n",
       "      <td>0.052</td>\n",
       "      <td>125</td>\n",
       "      <td>{'learning_rate': 0.052000000000000005, 'n_est...</td>\n",
       "      <td>-0.273112</td>\n",
       "      <td>-0.569925</td>\n",
       "      <td>-0.681151</td>\n",
       "      <td>...</td>\n",
       "      <td>0.193816</td>\n",
       "      <td>5</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.750</td>\n",
       "      <td>0.250</td>\n",
       "      <td>0.644444</td>\n",
       "      <td>0.213293</td>\n",
       "      <td>27848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7839</th>\n",
       "      <td>0.063560</td>\n",
       "      <td>0.001277</td>\n",
       "      <td>0.001533</td>\n",
       "      <td>0.000042</td>\n",
       "      <td>0.068</td>\n",
       "      <td>100</td>\n",
       "      <td>{'learning_rate': 0.068, 'n_estimators': 100}</td>\n",
       "      <td>-0.273675</td>\n",
       "      <td>-0.567905</td>\n",
       "      <td>-0.705889</td>\n",
       "      <td>...</td>\n",
       "      <td>0.190345</td>\n",
       "      <td>6</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.750</td>\n",
       "      <td>0.375</td>\n",
       "      <td>0.691667</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>9553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3042</th>\n",
       "      <td>0.062784</td>\n",
       "      <td>0.000799</td>\n",
       "      <td>0.001521</td>\n",
       "      <td>0.000027</td>\n",
       "      <td>0.027</td>\n",
       "      <td>100</td>\n",
       "      <td>{'learning_rate': 0.027000000000000003, 'n_est...</td>\n",
       "      <td>-0.462242</td>\n",
       "      <td>-0.584624</td>\n",
       "      <td>-0.601283</td>\n",
       "      <td>...</td>\n",
       "      <td>0.103198</td>\n",
       "      <td>7</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.750</td>\n",
       "      <td>0.250</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.220129</td>\n",
       "      <td>20042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3393</th>\n",
       "      <td>0.062645</td>\n",
       "      <td>0.000817</td>\n",
       "      <td>0.001481</td>\n",
       "      <td>0.000028</td>\n",
       "      <td>0.03</td>\n",
       "      <td>100</td>\n",
       "      <td>{'learning_rate': 0.030000000000000002, 'n_est...</td>\n",
       "      <td>-0.405477</td>\n",
       "      <td>-0.606294</td>\n",
       "      <td>-0.640579</td>\n",
       "      <td>...</td>\n",
       "      <td>0.122382</td>\n",
       "      <td>8</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.750</td>\n",
       "      <td>0.250</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.220129</td>\n",
       "      <td>20042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4565</th>\n",
       "      <td>0.094876</td>\n",
       "      <td>0.001898</td>\n",
       "      <td>0.001575</td>\n",
       "      <td>0.000037</td>\n",
       "      <td>0.04</td>\n",
       "      <td>150</td>\n",
       "      <td>{'learning_rate': 0.04, 'n_estimators': 150}</td>\n",
       "      <td>-0.285554</td>\n",
       "      <td>-0.617364</td>\n",
       "      <td>-0.692358</td>\n",
       "      <td>...</td>\n",
       "      <td>0.186864</td>\n",
       "      <td>9</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.750</td>\n",
       "      <td>0.375</td>\n",
       "      <td>0.669444</td>\n",
       "      <td>0.207573</td>\n",
       "      <td>19652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1291</th>\n",
       "      <td>0.125181</td>\n",
       "      <td>0.002050</td>\n",
       "      <td>0.001607</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>0.012</td>\n",
       "      <td>200</td>\n",
       "      <td>{'learning_rate': 0.012, 'n_estimators': 200}</td>\n",
       "      <td>-0.468924</td>\n",
       "      <td>-0.609551</td>\n",
       "      <td>-0.642298</td>\n",
       "      <td>...</td>\n",
       "      <td>0.088974</td>\n",
       "      <td>10</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.750</td>\n",
       "      <td>0.250</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.220129</td>\n",
       "      <td>20042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1996</th>\n",
       "      <td>0.173247</td>\n",
       "      <td>0.004348</td>\n",
       "      <td>0.001700</td>\n",
       "      <td>0.000041</td>\n",
       "      <td>0.018</td>\n",
       "      <td>275</td>\n",
       "      <td>{'learning_rate': 0.018000000000000002, 'n_est...</td>\n",
       "      <td>-0.339807</td>\n",
       "      <td>-0.590287</td>\n",
       "      <td>-0.642547</td>\n",
       "      <td>...</td>\n",
       "      <td>0.159188</td>\n",
       "      <td>11</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.625</td>\n",
       "      <td>0.250</td>\n",
       "      <td>0.663889</td>\n",
       "      <td>0.244444</td>\n",
       "      <td>20242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2695</th>\n",
       "      <td>0.126691</td>\n",
       "      <td>0.002265</td>\n",
       "      <td>0.001628</td>\n",
       "      <td>0.000015</td>\n",
       "      <td>0.024</td>\n",
       "      <td>200</td>\n",
       "      <td>{'learning_rate': 0.024, 'n_estimators': 200}</td>\n",
       "      <td>-0.384917</td>\n",
       "      <td>-0.583945</td>\n",
       "      <td>-0.703404</td>\n",
       "      <td>...</td>\n",
       "      <td>0.130986</td>\n",
       "      <td>12</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.750</td>\n",
       "      <td>0.250</td>\n",
       "      <td>0.644444</td>\n",
       "      <td>0.213293</td>\n",
       "      <td>27848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1528</th>\n",
       "      <td>0.171332</td>\n",
       "      <td>0.002224</td>\n",
       "      <td>0.001666</td>\n",
       "      <td>0.000027</td>\n",
       "      <td>0.014</td>\n",
       "      <td>275</td>\n",
       "      <td>{'learning_rate': 0.014000000000000002, 'n_est...</td>\n",
       "      <td>-0.387363</td>\n",
       "      <td>-0.599030</td>\n",
       "      <td>-0.646904</td>\n",
       "      <td>...</td>\n",
       "      <td>0.131902</td>\n",
       "      <td>13</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.625</td>\n",
       "      <td>0.250</td>\n",
       "      <td>0.619444</td>\n",
       "      <td>0.206679</td>\n",
       "      <td>32524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3861</th>\n",
       "      <td>0.062593</td>\n",
       "      <td>0.000824</td>\n",
       "      <td>0.001487</td>\n",
       "      <td>0.000030</td>\n",
       "      <td>0.034</td>\n",
       "      <td>100</td>\n",
       "      <td>{'learning_rate': 0.034, 'n_estimators': 100}</td>\n",
       "      <td>-0.441784</td>\n",
       "      <td>-0.548615</td>\n",
       "      <td>-0.659189</td>\n",
       "      <td>...</td>\n",
       "      <td>0.118922</td>\n",
       "      <td>14</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.750</td>\n",
       "      <td>0.250</td>\n",
       "      <td>0.688889</td>\n",
       "      <td>0.235309</td>\n",
       "      <td>9631</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1066</th>\n",
       "      <td>0.264650</td>\n",
       "      <td>0.003048</td>\n",
       "      <td>0.001782</td>\n",
       "      <td>0.000038</td>\n",
       "      <td>0.01</td>\n",
       "      <td>425</td>\n",
       "      <td>{'learning_rate': 0.010000000000000002, 'n_est...</td>\n",
       "      <td>-0.381971</td>\n",
       "      <td>-0.593391</td>\n",
       "      <td>-0.679527</td>\n",
       "      <td>...</td>\n",
       "      <td>0.137694</td>\n",
       "      <td>15</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.750</td>\n",
       "      <td>0.250</td>\n",
       "      <td>0.644444</td>\n",
       "      <td>0.213293</td>\n",
       "      <td>27848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5031</th>\n",
       "      <td>0.063471</td>\n",
       "      <td>0.001508</td>\n",
       "      <td>0.001529</td>\n",
       "      <td>0.000026</td>\n",
       "      <td>0.044</td>\n",
       "      <td>100</td>\n",
       "      <td>{'learning_rate': 0.044000000000000004, 'n_est...</td>\n",
       "      <td>-0.380761</td>\n",
       "      <td>-0.611077</td>\n",
       "      <td>-0.694085</td>\n",
       "      <td>...</td>\n",
       "      <td>0.141112</td>\n",
       "      <td>16</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.750</td>\n",
       "      <td>0.250</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.220129</td>\n",
       "      <td>20042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1056</th>\n",
       "      <td>0.109656</td>\n",
       "      <td>0.001659</td>\n",
       "      <td>0.001603</td>\n",
       "      <td>0.000023</td>\n",
       "      <td>0.01</td>\n",
       "      <td>175</td>\n",
       "      <td>{'learning_rate': 0.010000000000000002, 'n_est...</td>\n",
       "      <td>-0.479376</td>\n",
       "      <td>-0.604416</td>\n",
       "      <td>-0.640274</td>\n",
       "      <td>...</td>\n",
       "      <td>0.085980</td>\n",
       "      <td>17</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.750</td>\n",
       "      <td>0.250</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.220129</td>\n",
       "      <td>20042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8076</th>\n",
       "      <td>0.109031</td>\n",
       "      <td>0.001364</td>\n",
       "      <td>0.001582</td>\n",
       "      <td>0.000015</td>\n",
       "      <td>0.07</td>\n",
       "      <td>175</td>\n",
       "      <td>{'learning_rate': 0.07, 'n_estimators': 175}</td>\n",
       "      <td>-0.345884</td>\n",
       "      <td>-0.644488</td>\n",
       "      <td>-0.770121</td>\n",
       "      <td>...</td>\n",
       "      <td>0.147905</td>\n",
       "      <td>18</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.750</td>\n",
       "      <td>0.750</td>\n",
       "      <td>0.744444</td>\n",
       "      <td>0.081271</td>\n",
       "      <td>208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2228</th>\n",
       "      <td>0.153625</td>\n",
       "      <td>0.006548</td>\n",
       "      <td>0.002024</td>\n",
       "      <td>0.000369</td>\n",
       "      <td>0.02</td>\n",
       "      <td>225</td>\n",
       "      <td>{'learning_rate': 0.02, 'n_estimators': 225}</td>\n",
       "      <td>-0.353594</td>\n",
       "      <td>-0.574557</td>\n",
       "      <td>-0.712533</td>\n",
       "      <td>...</td>\n",
       "      <td>0.151401</td>\n",
       "      <td>19</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>0.750</td>\n",
       "      <td>0.375</td>\n",
       "      <td>0.647222</td>\n",
       "      <td>0.174271</td>\n",
       "      <td>27288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4097</th>\n",
       "      <td>0.093616</td>\n",
       "      <td>0.001284</td>\n",
       "      <td>0.001498</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>0.036</td>\n",
       "      <td>150</td>\n",
       "      <td>{'learning_rate': 0.036000000000000004, 'n_est...</td>\n",
       "      <td>-0.323361</td>\n",
       "      <td>-0.591987</td>\n",
       "      <td>-0.696943</td>\n",
       "      <td>...</td>\n",
       "      <td>0.165395</td>\n",
       "      <td>20</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.750</td>\n",
       "      <td>0.250</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.220129</td>\n",
       "      <td>20042</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20 rows Ã— 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
       "3745        0.078553      0.000611         0.001505        0.000028   \n",
       "8073        0.063099      0.001126         0.001582        0.000049   \n",
       "12870       0.071993      0.008178         0.001946        0.000462   \n",
       "5850        0.062990      0.000678         0.001561        0.000037   \n",
       "5968        0.079375      0.001292         0.001526        0.000033   \n",
       "7839        0.063560      0.001277         0.001533        0.000042   \n",
       "3042        0.062784      0.000799         0.001521        0.000027   \n",
       "3393        0.062645      0.000817         0.001481        0.000028   \n",
       "4565        0.094876      0.001898         0.001575        0.000037   \n",
       "1291        0.125181      0.002050         0.001607        0.000009   \n",
       "1996        0.173247      0.004348         0.001700        0.000041   \n",
       "2695        0.126691      0.002265         0.001628        0.000015   \n",
       "1528        0.171332      0.002224         0.001666        0.000027   \n",
       "3861        0.062593      0.000824         0.001487        0.000030   \n",
       "1066        0.264650      0.003048         0.001782        0.000038   \n",
       "5031        0.063471      0.001508         0.001529        0.000026   \n",
       "1056        0.109656      0.001659         0.001603        0.000023   \n",
       "8076        0.109031      0.001364         0.001582        0.000015   \n",
       "2228        0.153625      0.006548         0.002024        0.000369   \n",
       "4097        0.093616      0.001284         0.001498        0.000011   \n",
       "\n",
       "      param_learning_rate param_n_estimators  \\\n",
       "3745                0.033                125   \n",
       "8073                 0.07                100   \n",
       "12870               0.111                100   \n",
       "5850                0.051                100   \n",
       "5968                0.052                125   \n",
       "7839                0.068                100   \n",
       "3042                0.027                100   \n",
       "3393                 0.03                100   \n",
       "4565                 0.04                150   \n",
       "1291                0.012                200   \n",
       "1996                0.018                275   \n",
       "2695                0.024                200   \n",
       "1528                0.014                275   \n",
       "3861                0.034                100   \n",
       "1066                 0.01                425   \n",
       "5031                0.044                100   \n",
       "1056                 0.01                175   \n",
       "8076                 0.07                175   \n",
       "2228                 0.02                225   \n",
       "4097                0.036                150   \n",
       "\n",
       "                                                  params  \\\n",
       "3745       {'learning_rate': 0.033, 'n_estimators': 125}   \n",
       "8073        {'learning_rate': 0.07, 'n_estimators': 100}   \n",
       "12870      {'learning_rate': 0.111, 'n_estimators': 100}   \n",
       "5850   {'learning_rate': 0.051000000000000004, 'n_est...   \n",
       "5968   {'learning_rate': 0.052000000000000005, 'n_est...   \n",
       "7839       {'learning_rate': 0.068, 'n_estimators': 100}   \n",
       "3042   {'learning_rate': 0.027000000000000003, 'n_est...   \n",
       "3393   {'learning_rate': 0.030000000000000002, 'n_est...   \n",
       "4565        {'learning_rate': 0.04, 'n_estimators': 150}   \n",
       "1291       {'learning_rate': 0.012, 'n_estimators': 200}   \n",
       "1996   {'learning_rate': 0.018000000000000002, 'n_est...   \n",
       "2695       {'learning_rate': 0.024, 'n_estimators': 200}   \n",
       "1528   {'learning_rate': 0.014000000000000002, 'n_est...   \n",
       "3861       {'learning_rate': 0.034, 'n_estimators': 100}   \n",
       "1066   {'learning_rate': 0.010000000000000002, 'n_est...   \n",
       "5031   {'learning_rate': 0.044000000000000004, 'n_est...   \n",
       "1056   {'learning_rate': 0.010000000000000002, 'n_est...   \n",
       "8076        {'learning_rate': 0.07, 'n_estimators': 175}   \n",
       "2228        {'learning_rate': 0.02, 'n_estimators': 225}   \n",
       "4097   {'learning_rate': 0.036000000000000004, 'n_est...   \n",
       "\n",
       "       split0_test_neg_log_loss  split1_test_neg_log_loss  \\\n",
       "3745                  -0.341112                 -0.549322   \n",
       "8073                  -0.356271                 -0.657309   \n",
       "12870                 -0.228063                 -0.557090   \n",
       "5850                  -0.352012                 -0.573338   \n",
       "5968                  -0.273112                 -0.569925   \n",
       "7839                  -0.273675                 -0.567905   \n",
       "3042                  -0.462242                 -0.584624   \n",
       "3393                  -0.405477                 -0.606294   \n",
       "4565                  -0.285554                 -0.617364   \n",
       "1291                  -0.468924                 -0.609551   \n",
       "1996                  -0.339807                 -0.590287   \n",
       "2695                  -0.384917                 -0.583945   \n",
       "1528                  -0.387363                 -0.599030   \n",
       "3861                  -0.441784                 -0.548615   \n",
       "1066                  -0.381971                 -0.593391   \n",
       "5031                  -0.380761                 -0.611077   \n",
       "1056                  -0.479376                 -0.604416   \n",
       "8076                  -0.345884                 -0.644488   \n",
       "2228                  -0.353594                 -0.574557   \n",
       "4097                  -0.323361                 -0.591987   \n",
       "\n",
       "       split2_test_neg_log_loss  ...  std_test_neg_log_loss  \\\n",
       "3745                  -0.706908  ...               0.140328   \n",
       "8073                  -0.693475  ...               0.131424   \n",
       "12870                 -0.718041  ...               0.212997   \n",
       "5850                  -0.721012  ...               0.142894   \n",
       "5968                  -0.681151  ...               0.193816   \n",
       "7839                  -0.705889  ...               0.190345   \n",
       "3042                  -0.601283  ...               0.103198   \n",
       "3393                  -0.640579  ...               0.122382   \n",
       "4565                  -0.692358  ...               0.186864   \n",
       "1291                  -0.642298  ...               0.088974   \n",
       "1996                  -0.642547  ...               0.159188   \n",
       "2695                  -0.703404  ...               0.130986   \n",
       "1528                  -0.646904  ...               0.131902   \n",
       "3861                  -0.659189  ...               0.118922   \n",
       "1066                  -0.679527  ...               0.137694   \n",
       "5031                  -0.694085  ...               0.141112   \n",
       "1056                  -0.640274  ...               0.085980   \n",
       "8076                  -0.770121  ...               0.147905   \n",
       "2228                  -0.712533  ...               0.151401   \n",
       "4097                  -0.696943  ...               0.165395   \n",
       "\n",
       "       rank_test_neg_log_loss  split0_test_accuracy  split1_test_accuracy  \\\n",
       "3745                        1              0.888889              0.777778   \n",
       "8073                        2              0.888889              0.666667   \n",
       "12870                       3              0.888889              0.666667   \n",
       "5850                        4              0.888889              0.777778   \n",
       "5968                        5              0.888889              0.666667   \n",
       "7839                        6              1.000000              0.666667   \n",
       "3042                        7              0.888889              0.777778   \n",
       "3393                        8              0.888889              0.777778   \n",
       "4565                        9              1.000000              0.555556   \n",
       "1291                       10              0.888889              0.777778   \n",
       "1996                       11              1.000000              0.777778   \n",
       "2695                       12              0.888889              0.666667   \n",
       "1528                       13              0.888889              0.666667   \n",
       "3861                       14              0.888889              0.888889   \n",
       "1066                       15              0.888889              0.666667   \n",
       "5031                       16              0.888889              0.777778   \n",
       "1056                       17              0.888889              0.777778   \n",
       "8076                       18              0.888889              0.666667   \n",
       "2228                       19              0.888889              0.666667   \n",
       "4097                       20              0.888889              0.777778   \n",
       "\n",
       "       split2_test_accuracy  split3_test_accuracy  split4_test_accuracy  \\\n",
       "3745               0.444444                 0.750                 0.250   \n",
       "8073               0.666667                 0.750                 0.250   \n",
       "12870              0.666667                 0.750                 0.625   \n",
       "5850               0.666667                 0.625                 0.375   \n",
       "5968               0.666667                 0.750                 0.250   \n",
       "7839               0.666667                 0.750                 0.375   \n",
       "3042               0.666667                 0.750                 0.250   \n",
       "3393               0.666667                 0.750                 0.250   \n",
       "4565               0.666667                 0.750                 0.375   \n",
       "1291               0.666667                 0.750                 0.250   \n",
       "1996               0.666667                 0.625                 0.250   \n",
       "2695               0.666667                 0.750                 0.250   \n",
       "1528               0.666667                 0.625                 0.250   \n",
       "3861               0.666667                 0.750                 0.250   \n",
       "1066               0.666667                 0.750                 0.250   \n",
       "5031               0.666667                 0.750                 0.250   \n",
       "1056               0.666667                 0.750                 0.250   \n",
       "8076               0.666667                 0.750                 0.750   \n",
       "2228               0.555556                 0.750                 0.375   \n",
       "4097               0.666667                 0.750                 0.250   \n",
       "\n",
       "       mean_test_accuracy  std_test_accuracy  rank_test_accuracy  \n",
       "3745             0.622222           0.237398               32209  \n",
       "8073             0.644444           0.213293               27848  \n",
       "12870            0.719444           0.093953                2464  \n",
       "5850             0.666667           0.172357               20042  \n",
       "5968             0.644444           0.213293               27848  \n",
       "7839             0.691667           0.200000                9553  \n",
       "3042             0.666667           0.220129               20042  \n",
       "3393             0.666667           0.220129               20042  \n",
       "4565             0.669444           0.207573               19652  \n",
       "1291             0.666667           0.220129               20042  \n",
       "1996             0.663889           0.244444               20242  \n",
       "2695             0.644444           0.213293               27848  \n",
       "1528             0.619444           0.206679               32524  \n",
       "3861             0.688889           0.235309                9631  \n",
       "1066             0.644444           0.213293               27848  \n",
       "5031             0.666667           0.220129               20042  \n",
       "1056             0.666667           0.220129               20042  \n",
       "8076             0.744444           0.081271                 208  \n",
       "2228             0.647222           0.174271               27288  \n",
       "4097             0.666667           0.220129               20042  \n",
       "\n",
       "[20 rows x 23 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gsearch_result.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3745     0.622222\n",
       "8073     0.644444\n",
       "12870    0.719444\n",
       "5850     0.666667\n",
       "5968     0.644444\n",
       "           ...   \n",
       "29586    0.619444\n",
       "33249    0.650000\n",
       "34539    0.700000\n",
       "34500    0.630556\n",
       "34179    0.652778\n",
       "Name: mean_test_accuracy, Length: 35100, dtype: float64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gsearch_result['mean_test_accuracy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3745     32209\n",
       "8073     27848\n",
       "12870     2464\n",
       "5850     20042\n",
       "5968     27848\n",
       "         ...  \n",
       "29586    32524\n",
       "33249    24445\n",
       "34539     3014\n",
       "34500    28522\n",
       "34179    21022\n",
       "Name: rank_test_accuracy, Length: 35100, dtype: int32"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gsearch_result['rank_test_accuracy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3745    -0.596133\n",
       "8073    -0.608179\n",
       "12870   -0.611247\n",
       "5850    -0.614984\n",
       "5968    -0.619903\n",
       "           ...   \n",
       "29586   -3.528528\n",
       "33249   -3.534978\n",
       "34539   -3.559429\n",
       "34500   -3.560073\n",
       "34179   -3.623068\n",
       "Name: mean_test_neg_log_loss, Length: 35100, dtype: float64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gsearch_result['mean_test_neg_log_loss']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "obj_det_clone",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7e782c4fbc36884394c9170738cdc99c03bcf42c881b0742568b6d97ce29ddb2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
